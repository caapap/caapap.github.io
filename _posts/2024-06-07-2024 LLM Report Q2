---
layout: post
---
2024 LLM Report Q2

***\*1\*******\*引言\****
整理大模型知识的主要目的在于提供一个全面的技术视角，以帮助团队成员深入理解当前和未来人工智能领域的关键趋势和技术进展。

通过系统地总结和分析大模型技术，我们不仅能够加强团队内部的技术共识，增强成员之间的知识共享，而且能够显著提升团队在面对复杂问题时的解决方案设计能力、促进团队整体的创新能力、提高项目交付的执行效率，从而在竞争激烈的市场环境中保持领先。

本报告专注于覆盖当前至未来一段时间内的大模型技术。具体来说，报告将涵盖最新的深度学习架构、优化算法、以及如何在各种应用中部署这些模型的策略。技术范畴包括但不限于卷积神经网络、变换器模型（Transformer）、多模态MLLM、增强检索生成（RAG）、智能体（Agent）等。此外，报告还将探讨这些技术在特定行业中的应用案例，如自然语言处理和图像识别、知识库问答、智能体等领域。

时间周期上，报告将回顾过去一个季度的技术发展，并展望未来一至两年内的技术趋势和潜在挑战。这样的时间安排旨在为团队提供一个及时更新的技术视角，同时保持足够的前瞻性，以便团队可以据此做出战略性的长远规划。





# ***\*2\*******\*大模型技术概述\****

## ***\*2\*******\*.\*******\*1\*******\*历史与背景\****

### **2****.****1****.****1****神经网络的发展历程**

**1.** ***\*1958\*******\*年：感知机\****

1958 年，弗兰克 · 罗森布拉特发明了***\*感知机\****，这是一种非常简单的机器模型，后来成为当今智能机器的核心和起源。***\*感知机\****是一个非常简单的二元分类器，可以确定给定的输入图像是否属于给定的类。为了实现这一点，它使用了单位阶跃***\*激活函数\****。使用单位阶跃***\*激活函数\****，如果输入大于0，则输出为1，否则为0。

![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps50.jpg) 

图 1 感知机算法

而其意图并不是将***\*感知\****机构建为算法，而是构建成一种机器。***\*感知机\****是在名为 Mark I ***\*感知\****机的硬件中实现的。Mark I ***\*感知\****机是一台纯电动机器。它有 400 个光电管（或光电探测器），其权重被编码到电位器中，权重更新（发生在反向传播中）由电动机执行。

![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps51.jpg) 

图 2 Mark I感知机

Mark I 感知机的目标仅仅是识别图像，而当时它只能识别两个类别。人们花了一些时间才知道添加更多层（感知机是单层神经网络）可以使网络具有学习复杂功能的能力。这进一步产生了多层感知机 (***\*MLP\****)。

**2.** ***\*1980\*******\*年：卷积神经网络\*******\*（\*******\*CNN\*******\*）\****

卷积神经网络（Convolutional Neural Network, ***\*CNN\****）是一种专为处理具有已知网格状拓扑的数据（例如图像）设计的深度学习架构。***\*CNN\**** 是机器学习和计算机视觉领域中的一种基础技术，广泛应用于图像和视频识别、推荐系统、图像分类、医学图像分析、自然语言处理和金融时间序列等。

以下为CNN卷积网络的模型：

![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps52.jpg) 

图 3 CNN卷积网络模型

CNN的核心思想是利用卷积层来自动提取输入数据中的特征，无需手动设计特征提取算法。卷积层通过滤波器（或称为卷积核）在输入数据上进行滑动窗口操作，计算窗口内数据与滤波器的element-wise（两个矩阵对应位置元素）乘积之和，生成特征图（feature map）。这个过程通过捕捉***\*局部特征\****，逐渐抽象出数据的***\*高级语义信息\****。

卷积网络CNN的典型架构包括以下几个层次：

**l** ***\*卷积层\****：通过多个不同的滤波器***\*对输入图像进行卷积操作，提取不同的特征\****。每个滤波器负责从输入数据中检测不同的特征。由于使用了局部感知区和参数共享技术，卷积层能有效减少网络参数的数量，降低模型的复杂度。

**l** ***\*激活层\****：卷积层后通常会跟一个非线性激活层，最常用的是ReLU修正线性单元（Rectified Linear Unit）激活函数。激活层的主要作用是***\*引入非线性因素\****，解决网络无法表达复杂模式的问题。

**l** ***\*池化层\****（Pooling Layer）：池化层用于***\*降低特征图的空间维度\****（高度和宽度），增强模型的泛化能力（指模型能够在不同的数据集上进行预测或分类，而不会因为训练数据的特殊性质而产生偏差），减少计算量。最常见的池化操作有最大池化和平均池化，它们分别取区域内的最大值和平均值作为该区域的代表。

**l** ***\*全连接层\****（Fully Connected Layers）：在多个卷积和池化层之后，网络会包含一个或多个全连接层，其目的是***\*将前面卷积层提取的局部特征综合起来\****完成最终的任务（如***\*分类\****、***\*回归\****等）。

**l** ***\*输出层\****：根据具体的应用，输出层可以设计为回归或分类层。例如，在图像分类任务中，输出层通常使用***\*softmax\*******\*函数\****（将任意实数映射为概率分布，并可以处理多分类问题）来输出每个类别的概率，最后选择概率最高的类别作为最终的分类结果。

**3.** ***\*1982\*******\*~\*******\*1986\**** ***\*: 循环神经网络 (\*******\*RNN\*******\*)\****

在多层感知机和CNN显示出解决图像识别问题的潜力之后，人们开始思考如何对***\*文本等序列数据\****进行建模。

***\*循环神经网路是一类旨在处理序列的神经网路\****。与多层感知机（MLP）等前馈网络不同，RNN有一个内部反馈回路，负责记住每个时间步的信息状态。

![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps53.jpg) 

图 4 前馈网络与循环神经网络

循环神经网络是一类旨在处理序列的神经网络。与多层感知机 (MLP) 等前馈网络不同，RNN 有一个内部反馈回路，负责记住每个时间步的信息状态。

**4.** ***\*1998\*******\*：第一个卷积神经网络架构\*******\*，\*******\*LeNet\*******\*-\*******\*5\****

1998年，现代卷积神经网络的基本结构***\*LeNet\*******\*-\*******\*5\****（基于CNN）结构，由图灵奖获得者Yann LeCun于1998年提出。它包括***\*两个卷积层、两个池化层和三个全连接层\****。LeNet-5的特点是结构简单，易于实现，并且在图像分类任务上取得了很好的效果。而机器学习方法也由早期基于浅层机器学习的模型，变为了基于深度学习的模型，为自然语言生成、计算机视觉等领域的深入研究奠定了基础，对后续深度学习框架的迭代及大模型发展具有开创性的意义。

**5.** ***\*探索沉淀期（\*******\*2006\*******\*年-\*******\*2019\*******\*年）：以\*******\*Transformer\*******\*为代表的全新神经网络模型阶段\****

2013年，自然语言处理模型***\*Word2Vec\****诞生，首次提出将单词转换为向量的“***\*词向量模型\****”，以便计算机更好地理解和处理文本数据。

2014年，被誉为21世纪最强大算法模型之一的***\*GAN\****（对抗式生成网络）诞生，标志着深度学习进入了生成模型研究的新阶段。

2017年，Google颠覆性地提出了基于自注意力机制的神经网络结构***\*Transformer\****架构，奠定了大模型预训练算法架构的基础。***\*Transformer\****的设计摒弃了传统的循环神经网络（***\*RNN\****）和长短期记忆网络（***\*LSTM\****）结构，转而全面采用了注意力机制（***\*Attention\**** ***\*Mechanism\****），这使得模型能够更有效地处理序列数据，并显著提高了处理长距离依赖问题的能力。

2018年，OpenAI和Google分别发布了GPT-1与BERT大模型，意味着预训练大模型成为自然语言处理领域的主流。在探索期，以Transformer为代表的全新神经网络架构，奠定了大模型的算法架构基础，使大模型技术的性能得到了显著提升。

**6.** ***\*迅猛发展期（\*******\*2020\*******\*年-至今）：以\*******\*GPT\*******\*为代表的预训练大模型阶段\****

2020年，OpenAI公司推出了GPT-3，模型参数规模达到了175B，需要700GB来存储，为有史以来参数最多的神经网络模型。该模型在许多任务上展示了强大的零样本和少样本的能力。随后，更多策略如基于人类反馈的强化学习（RHLF）、代码预训练、指令微调等开始出现，被用于进一步提高推理能力和任务泛化。2022年11月，搭载了GPT3.5的ChatGPT横空出世，凭借逼真的自然语言交互与多场景内容生成能力，迅速引爆互联网。2023年3月，最新发布的超大规模多模态预训练大模型——GPT-4，具备了多模态理解与多类型内容生成能力。在迅猛发展期，大数据、大算力和大算法完美结合，大幅提升了大模型的预训练和生成能力以及多模态多场景应用能力。而ChatGPT的巨大成功，就是在微软Azure强大的算力以及维基百科等海量数据支持下，在Transformer架构基础上，沿用GPT模型及人类反馈的强化学习（RLHF），并进行精调的策略下取得的。

### **2****.****1****.****2****语言模型的发展历程**

一般来说，语言模型旨在对于人类语言的内在规律进行建模，从而准确预测词序列中***\*未来（或缺失）词\****或***\*词元（\*******\*Token\*******\*）\****的概率。根据所采用技术方法的不同，针对语言模型的研究工作可以分为以下四个主要发展阶段：

![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps54.jpg) 

图 5 基于任务求解能力的四代语言模型的演化过程（图片来源[1]）

**1.** ***\*统计语言模型\****（Statistical Language Model, SLM）.统计语言模型[2][3]是根据词序列中若干个连续的上下文单词来预测下一个词的出现概率，即根据一个固定长度的前缀来预测目标单词。SLM具备一定生成能力，可以辅助解决部分任务，但因为受数据稀疏问题影响严重，无法精确建模复杂的高阶语义关系。

**2.** ***\*神经语言模型\****（Neural Language Model, NLM）.神经语言模型[4][5]使用神经网络（如RNN）来建模文本序列的生成。神经语言模型借助***\*分布式词表示\****（Distributed Word Representation）这一概念，有效克服了SLM中的数据稀疏问题。与基于词典空间的***\*稀疏词向量\****不同***\*，\****分布式词表示因使用***\*低维稠密向量\****来表示词汇的语义，能够刻画更为丰富的***\*隐含语义特征\****。同时，稠密向量的非零表征（大部分元素值非零）对于复杂语言模型的搭建也非常友好。分布式词向量又称为“词嵌入”（Word Embedding）。这种语言建模方法为自然语言理解任务提供了较为通用的解决途径，词嵌入模型word2vec就是一典型代表。

**3.** ***\*预训练语言模型\****（Pre-trained Language Model, PLM）.与早期的词嵌入模型相比，预训练语言模型在训练架构与训练数据两个方面进行了改进和创新。数据上使用大量的无标注数据进行双向LSTM（Bidirectional LSTM，biLSTM）网络训练，完成后即可获得***\*学习上下文感知\****的单词表示，实现不同于word2vec学习固定的词表示方式。进一步，可以根据下游任务数据对biLSTM网络进行微调（Fine-Tuning），从而实现面向特定任务的模型优化。然而，传统序列神经网络的***\*长文本建模能力较弱\****，并且不容易并行训练，这些缺点限制了早期预训练模型（如ELMo）的性能。直到2017年谷歌提出了***\*基于自注意力机制（\*******\*Self\*******\*-\*******\*Attention\*******\*）的\**** ***\*Transformer\**** ***\*模型\*******\*[\*******\*6\*******\*]\****，通过自注意力机制建模长程序列关系。Transformer的一个主要优势就是其模型设计对于硬件非常友好，可以通过GPU或者TPU进行加速训练，这为研发大语言模型提供了可并行优化的神经网络架构。基于Transformer架构，谷歌进一步提出了预训练语言模型BERT，采用了***\*仅有编码器\*******\*Encoder\*******\*的\*******\*Transformer\*******\*架构\****，并通过在***\*大规模无标注数据\****上使用专门设计的预训练任务来学习双向语言模型。在同期，OpenAI也迅速采纳了Transformer架构，将其用于 GPT-1的训练。与BERT模型不同的是，GPT-1采用了仅有Decoder的Transformer架构，以及基于下一个词元预测的预训练任务进行模型的训练。一般来说，***\*编码器\*******\*Encoder\*******\*架构被认为更适合去解决自然语言理解任务（如完形填空等）\****，而***\*解码器\*******\*Decoder\*******\*架构更适合解决自然语言生成任务（如文本摘要等）\****。以ELMo、BERT、GPT-1为代表的预训练语言模型确立了“预训练-微调”这一任务求解范式。其中，预训练阶段旨在通过大规模无标注文本建立模型的基础能力，而微调阶段则使用有标注数据对于模型进行特定任务的适配，从而更好地解决下游的自然语言处理任务。

**4.** ***\*大语言模型\****（Large Language Model, LLM）.研究人员发现，通过规模扩展（如增加***\*模型参数规模\****或***\*数据规模\****）通常会带来下游任务（指使用预训练的模型来执行特定任务，如文本分类、问答系统）的模型性能提升。在尝试训练更大规模的预训练语言模型（如175B的GPT-3和540B的PaLM）后，大规模的预训练语言模型在解决复杂任务时表现出了不同的行为。如GPT-3可通过“***\*上下文学习\****”（In-Context Learning, ICL）的方式来利用少样本数据解决下游任务，而GPT-2则不具备这一能力，即“***\*涌现能力\****”（Emergent Abilities）。为了定义这种差别，大型预训练语言模型被命名为“大语言模型”（Large Language Model，LLM）[7]。作为大语言模型的一个代表性应用，ChatGPT 将 GPT 系列大语言模型适配到对话任务中，展现出令人震撼的人机对话能力，一经上线就取得了社会的广泛关注。

通过回顾上述发展历程，可以看到语言模型并不是一个新的技术概念，而是历经了长期的发展历程。早期的语言模型主要面向自然语言的建模和生成任务，而最新的语言模型（如 GPT-4、星火大模型v3.5）则侧重于复杂任务的求解。随着***\*模型参数\****、***\*训练数据\****、***\*计算算力\****的大规模扩展，最新一代大语言模型的任务求解能力有了显著提升，能够不再依靠下游任务数据的微调进行通用任务的求解，可以解决的任务范围得到了极大扩展，所获得的任务性能得到了显著提高，这是人工智能历史上的一次重要进步。

## ***\*2\*******\*.\*******\*2\*******\*术语与概念\****

l AGI：通用人工智能（artifical general intelligence），指具有高效的学习和泛化能力、能够根据所处的复杂动态环境自主产生并完成任务的通用人工智能体。

l 大模型：通常指的是***\*LLM\****（large language model ），具有大规模参数、大量训练数据的复杂计算模型。如国内的讯飞星火大模型v3.5、文心一言，国外的ChatGPT-4、Llama3 400B、Gemini Ultra等。

l 显卡算力：显卡算力是指显卡GPU的计算能力。一般使用***\*FLOPS\****（每秒浮点运算次数）来评估显卡的性能。显卡算力=核心数量*时钟速度*浮点运算单元数量。例如：半精度***\*FP16\****性能，T4卡的为65TF（每秒65万次浮点运算），A100卡为312TF，Atlas300I卡为44TF，昇腾910B卡为376TF。

l 推理：使用训练好的模型来对新输入的数据进行分析、分类和预测。推理依靠模型在训练过程中学习到的知识来对新数据做出判断和预测。

l 思维链：这是一种用大模型解决复杂问题的方法。它通过让LLM将一个复杂问题分解为多个小步骤，然后逐步分析每个小步骤得到正确的答案。思维链可以看成是一种指令微调。比如：解数学题。

l Prompt：“提示词”，Prompt可以简单理解为给语言模型提供的一段初使文字或句子,用于指导和引导模型的后续生成。让它可以根据这个开端和引导产生更加符合上下文和目的的输出内容。

l Token：指语言模型中用来表示单词或短语的符号。大模型处理文本时，会先将文本分割成一个个最小单位的词、字根或短语，这些就是token。比如：“我很开心”可以分成“我”,“很”, “开心”三个 token;“I am happy”可能被分成“I”，“am”，”hap”，”py”四个token。5 bytes =1 token ；1 web page=0.1MB ;1 book =0.8MB。

l Agents：Agent(智能体)是一个设置了一些目标或任务，可以迭代运行的大型语言模型。这与在LLM（如星火大模型***\*）\****中通用使用方式不同。在讯飞星火大模型中，你提出一个问题将获得一个答案作为回应。而Agent拥有复杂的工作流程，模型本质上可以自我对话，而无需人类驱动每一部分的交互。

l RAG（Retrieval-Augmented Generation）：检索增强生成，一种结合了***\*检索\****和***\*生成\****的自然语言处理模型。适用于那些标准预训练模型可能无法直接回答的***\*复杂查询。\****

**注：以下为专业名词解释**

l Transformer：Transformer架构是目前语言大模型采用的主流架构，于2017年由Google提出在"Attention is All You Need"这篇论文中被提出，现在已经成为了许多语言大模型的基础架构。Transformer架构的主要特点是使用了***\*自注意力机制\****（Self-Attention Mechanism），这使得***\*模型在处理序列数据时，能够考虑到序列中所有元素之间的关系，而不仅仅是相邻元素之间的关系\****[1]***\*。\****这使得Transformer模型在处理***\*长距离依赖问题\****时，比传统的RNN和CNN模型有更好的性能。Transformer模型由两部分组成：***\*编码器\****（***\*Encoder\****）和***\*解码器\****（***\*Decoder\****）。编码器负责把输入序列转换成一种连续的表示，解码器则负责把这种表示转换成输出序列。常用模型包括只用Encoder的模型（BERT、RoBERTa、ALBERT等），也包括只用Decoder的模型（GPT1、2、3、ChatGPT等），还有适用Encoder+Decoder结构的（T5、BART等）。

![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps55.jpg) 

图 6 Transformer架构的编码器-解码器结构，选自《Attention Is All You Need》

l NLP：***\*NLP\****（CNN、DNN、RNN）是人工智能和语言学领域的分支学科。此领域探讨如何处理及运用自然语言；***\*自然语言处理\****包括多方面和步骤，基本有认知、理解、生成等部分。***\*CNN\*******\*、\*******\*DNN\*******\*、\*******\*RNN\****是深度学习中常用的神经网络结构。这三种神经网络结构在不同的领域和任务中各有优势，通过它们的组合和改进，可以实现更加复杂和强大的深度学习模型。

l End-to-End learning：是一种机器学习方法，它旨在通过一个连续的神经网络模型实现多个任务，而无需在中间进行任何特征工程或数据转换。这种方法可以大大简化训练和预测过程，提高模型性能和效率。

![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps56.jpg) 

图 7 端到端学习结构

l 无监督学习（Unsupervised learning）：无监督学习是机器学习的一种方法，没有给定事先标记过的训练示例，自动对输入的资料进行分类或分群。主要运用聚类分析、关系规则、维度缩减[2]。

![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps57.jpg) 

图 8 无监督学习

l 监督学习（Supervised learning）：监督学习也是机器学习的一种方法，可以由训练资料中学到或创建一个模式(函数/learning model),并依此模式推测新的实例。训练资料是由输入对象(通常是向量)和预期输出所组成[2]。

l 强化学习（Reinforcement Learning）：人工智能的一种学习方法，它通过***\*让算法与环境交互\****并试图最大化某种奖励信号来学习如何在环境中实现目标。在强化学习过程中，学习主体或智能体（agent）不断从环境中获取状态，采取行动，接收***\*奖励\****（或***\*惩罚\****）并调整其策略。通过这种方式，智能体学习如何根据环境状态选择最佳行动以最大化累积奖励。通过外部获得激励来校正学习方向从而获得一种自适应的学习能力。可用于自动驾驶、游戏、机器人技术、供应链优化、电力系统管理等场景。

l Embedding：嵌入，是一种将***\*文本\****映射为***\*数字向量\****的技术。这种映射让计算机可以利用向量间的关系来理解词语的语义和语法。比如，使用余弦相似度、欧式距离等文本相似度算法，计算两个向量之间的距离，来判断词义之间的亲近程度。

l 参数量：6B、13B、70B，指模型的规模，参数的个数，B是Billion/十亿的意思，小模型如13B的模型可以在消费级显卡上运行，而大模型的参数量一般非常大（如GPT-3的175B参数量），这也是需要大量数据和强大算力的原因之一。***\*在机器学习中，参数是模型用于进行预测的内部变量。\****

l 预训练（pre-training）：深度学习模型训练过程的第一个阶段，模型在大规模的***\*未标注数据集\****上进行训练，学习到一些通用的语言表示。例如，星火大模型在预训练阶段会学习到词汇的语义、语法规则、一些常识等。***\*预训练阶段使用的是未标注数据，目标是学习通用的语言表示\****。

l 微调（Fine-tuning）：深度学习模型训练过程的第二个阶段，也被称为***\*Instruct\****阶段，模型在特定任务的***\*标注数据集\****上进行训练，这个数据集通常包含了一些输入和对应的目标输出，模型需要学习如何根据输入生成目标输出。例如，输入可能是***\**"生成一个\**\******\**Python\**\******\**函数，这个函数接受一个整数列表作为参数，返回这个列表中的最大值"\**\***，目标输出则是对应的Python代码。在训练过程中，模型会尝试生成与目标输出尽可能接近的输出，并根据生成的输出和目标输出之间的差距来调整模型的参数。这个过程通常使用一种叫做***\*梯度下降\****的优化算法。通过这种方式，模型可以学习到如何根据指令生成代码、如何解释代码等任务相关的知识。这就是预训练模型在微调阶段进行instruct的方式。***\*微调阶段使用的是标注数据，目标是学习完成特定任务\****。

l Zero-shot learning：零样本学习（ZSL）是深度学习中的一个解决方案，适用于在实际模型测试时，当模型观察到未在训练期间出现的样本类别，并需要预测它们所属的类别的场景。零样本方法通常用辅助信息将观察到的和未观察到的类别联系起来，该辅助信息对可观察对象的区别特性进行了相应的编码[15]。例如，给定一组动物图像以进行分类，以及描述动物外观的辅助文本描述，一个已经被训练识别马但从未被给予斑马的人工智能模型仍然可以在知道斑马看起来像带条纹的马时识别斑马。





# ***\*3\*******\*大模型应用及\*******\*技术创新\****

目前业界大模型技术发展可以归类为以下几种路线：

l 大模型本体：备案上线的中国大模型、知名大模型和大模型应用。

l 工具与平台：LLMOps、大模型聚合平台、开发工具。

l 算力：英伟达、AMD、华为晟腾、昆仑芯等硬件算力平台。

l 基础设施：向量数据库、数据库向量支持、大模型框架、微调（Fine Tuning）、大模型训练平台与工具。

l AI编程：插件、IDE、终端，代码生成工具。

l LLM Agent：星火Agent平台、微软JARVIS、AutoGPT等。

l 编程语言：Python、C++、Mojo、JavaScript等。

LLM技术图谱参考如下：

![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps58.jpg) 

图 9 大模型技术图谱

## ***\*3\*******\*.\*******\*1\*******\*大模型基础设施\****

### **3****.****1****.****1****向量数据库/数据库向量支持**

向量数据库是专门用于存储和检索向量数据的数据库，它可以为 LLM 提供高效的存储和检索能力。通过数据向量化，实现了在向量数据库中进行高效的相似性计算和查询。根据向量数据库的的实现方式,可以将向量数据库大致分为两类：

1. 原生向量数据库

原生的向量数据库专门为存储和检索向量而设计，所管理的数据是基于对象或数据点的向量表示进行组织和索引。包括：Chroma、LanceDB、Margo、Milvus、Pinecone等均属于原生向量数据库。

![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps59.jpg) 

图 10 原生向量数据库

2. 添加向量支持的传统数据库

除了选择专业的向量数据库，对传统数据库添加“向量支持”也是主流方案。比如Redis、PostgreSQL、ClickHouse、Elasticsearch等传统数据库均已支持向量检索。

![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps60.jpg) 

图 11 添加向量支持的传统数据库

### **3****.****1****.****2****大模型框架及微调（****Fine** **Tuning****）**

大模型框架指专门设计用于构建、训练和部署大型机器学习模型和深度学习模型的软件框架。这些框架提供了必要的工具和库，使开发者能够更容易地处理大量的数据、管理巨大的网络参数量，并有效地利用硬件资源。微调（Fine Tuning）是在大模型框架基础上进行的一个关键步骤。在模型经过初步的大规模预训练后，微调是用较小、特定领域的数据集对模型进行后续训练，以使其更好地适应特定的任务或应用场景。这一步骤使得通用的大型模型能够在特定任务上表现出更高的精度和更好的效果。大模型框架提供了 LLM 的基本能力和普适性，而微调则是实现特定应用和优化性能的关键环节。两者相结合，使得 LLM 在广泛的应用场景中都能发挥出色的性能。

![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps61.jpg) 

图 12 大模型框架和微调工具

大模型框架特点如下：

l 抽象和简化：大模型开发框架通过提供高层次的API简化了复杂模型的构建过程。这些 API 抽象掉了许多底层细节，使开发者能 够专注于模型的设计和训练策略。

l 性能优化：这些框架经过优化，以充分利用GPU、TPU等高性能计算硬件，以加速模型的训练和推理过程。

l 易于扩展：为了处理大型数据集和大规模参数网络，这些框架通常设计得易于水平扩展，支持在多个处理器或多个服务器上并行处理。

l 支持大数据集：它们提供工具来有效地加载、处理和迭代大型数据集，这对于训练大型模型尤为重要。

![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps62.jpg) 

图 13 国产深度学习框架OneFlow

想要微调一个模型，一般包含以下关键步骤：

1. 选择预训练模型：选取一个已经在大量数据上进行过预训练的模型作为起点； 
2. 准备任务特定数据：收集与目标任务直接相关的数据集，这些数据将用于微调模型； 
3. 微调训练：在任务特定数据上训练预训练的模型，调整模型参数以适应特定任务； 
4. 评估：在验证集上评估模型性能，确保模型对新数据有良好的泛化能力； 
5. 部署：将性能经验证的模型部署到实际应用中去。

 

![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps63.jpg) 

图 14 微调的过程

### **3****.****1****.****3****大模型训练平台与工具**

大模型训练平台和工具提供了强大且灵活的基础设施，使得开发和训练复杂的语言模型变得可行且高效。这些工具提供了先进的算法、预训练模型和优化技术，极大地简化了模型开发过程，加速了实验周期，并使得模型能够更好地适应各种不同的应用场景。此外，它们还促进了学术界和工业界之间的合作与知识共享，推动了自然语言处理技术的快速发展和广泛应用。相比前边的大模型框架和微调，一言以蔽之：平台化（训练平台）、灵活化（各种工具）

![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps64.jpg) 

图 15 训练平台与工具

大模型训练平台与工具根据其性质不同，可分为以下几类：

1. 云服务和商业平台：这些平台提供了从模型开发到部署的综合解决方案，包括计算资源、数据存储、模型训练和部署服务。它们通常提供易于使用的界面，支持快速迭代和大规模部署。Amazon SageMaker、Google Cloud AI Platform 和 Microsoft Azure Machine Learning都是提供端到端机器学习服务的云平台。
2. 专用硬件加速工具：这些工具和库专门为加速机器学习模型的训练和推理而设计，通常利用GPU或TPU等硬件。这类工具可以显著提高训练和推理的速度，使得处理大规模数据集和复杂模型变得可行。NVIDIA CUDA和Google Cloud TPU 均是此类工具。
3. 开源框架和库：这类工具通常由开源社区支持和维护，提供了灵活、可扩展的工具和库来构建和训练大型机器学习模型，如TensorFlow 和PyTorch、Tinygrad、Hugging Face Transformers等。

### **3****.****1****.****4****大模型编程语言**

LLM 的训练和应用通常使用多种编程语言，取决于任务的需求和团队的偏好。

Python是LLM开发中最常用的编程语言。它的广泛使用得益于其简洁的语法、强大的库支持（如NumPy，Pandas，Matplotlib）和深度学习框架（如TensorFlow，PyTorch，Keras）

此外，AI开发领域也有崛起的新秀语言Mojo，C++有时用于优化计算密集型任务，而Java在企业环境中处理模型部署和系统集成方面常见。JavaScript适用于Web环境的LLM应用。

Mojo由LLVM/Swift之父创业公司Modular AI开发，于2023年9月面向大众开放，它结合了Python的易用性以及C语言的可移植性和性能。支持与任意Python代码无缝集成，性能是Python的68000倍。

## ***\*3\*******\*.\*******\*2\*******\*大模型应用现状\****

2022 年底大模型应用ChatGPT 发布后，点燃了世界范围内对于大模型技术及其应用的关注和热情。2023年，国内外各大厂商均投身于大模型的浪潮当中，涌现了诸多知名的大模型及应用，它们结合了文本、图片、视频、音频多种介质，在文本生成、图片生成、AI 编程等方向均有出色的表现。

![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps65.jpg) 

图 16 知名大模型

### **3****.****2****.****1****知名大模型**

在全球范围内，已经发布了多款知名大模型，这些大模型在各个领域都取得了突破性的进展。处理文本数据通用大模型的 GPT-4、讯飞星火v3.5、Gemini Ultra，能同时处理和理解多种类型数据的多模态模型 DALL-E 3，支持文本200万长度的Kimi Chat以及开源大模型的代表Llama 3都在短时间内获得了大量关注和用户，构成了大模型领域的“第一梯队”。

#### **3****.****2****.****1****.****1****星火大模型****V3****.****5**

***\*版本更新\****

2024年4月26日，讯飞星火大模型 V3.5 春季上新，首次支持***\*长文本、长图文、长语音\****，并首发***\*星火图文识别大模型\****，能够快速识别和学习多类型海量知识，行业场景下的回答更专业、更精准[14]。

***\*星火合同助手\****首次上线，审核规避合同风险、提炼总结关键要素、一键起草合同文件。星火智能评标助手升级，投标文件解析更高效，让评标更便捷、更高效、更准确。

首发“***\*多情感超拟人合成\****”和“***\*一句话声音复刻\****”，实现更生动、更具个性化的表达，带来更有温度的人工智能体验。

***\*星火智能体平台\****全新发布，解决大模型企业落地最后一公里，为员工打造专属助理，为企业解放生产力。

***\*测评分析\****

l 下载量排名第一***\*.\**** 讯飞星火APP安卓下载量超9600万次，国内工具类通用大模型APP排名第一。

l 领先GPT-4 Turbo. 垂直领域的知识问答星火总体水平超GPT-4 Turbo，目前星火大模型通用长文本能力，包括长文档信息抽取、长文档知识问答、长文档归纳总结、长文档文本生成等，总体已经达到GPT-4 Turbo今年4月最新长文本版本的97%的水平，而在多个垂直领域的知识问答任务上，星火大模型长文本总体水平已经超过GPT-4 Turbo。

l 长文本能力全新升级. 在技术层面上，不仅把大模型在多个领域做到业界最优，还进行了非常重要的“剪枝”和“蒸馏”，从而推出了业界最高性能的13B，也就是130亿的模型来处理长文本。在效果损失仅3%以内的情况下，使得讯飞星火在文档上传解析、知识问答的首响时间以及文字生成上都获得极大的效率提升，在保障长文本效果的情况下，无论是10K、64K、128K token，还是更长的文本，星火大模型的上述性能都是业界最优的。

l 图文识别大模型效果领先. 图文识别覆盖了31个最常见的典型场景，支持教育类的书刊、学术论文、专利、报纸、海报、产品白皮书、PPT和菜单等，理解力和易用性大幅提升。并且，对于上述场景中最常见的18种版面要素可以进行非常快速的识别和处理。例如页眉、页脚、标题、栏目、段落、表格、插图等等要素，甚至还包括比较难、但很实用的公式、印章、二维码、手写材料等。在国际公开的英文测试集，在科研、金融以及企业产品技术文档等等的识别效果在业界均处于领先位置[14]。

#### **3****.****2****.****1****.****2** **GPT****-****4**

2023年3月14日，OpenAI发布了GPT-4，向科技界再次扔下了一枚“核弹”。在过去一年多的时间，GPT-4在各项能力指标上一直是领先业界的水平，也是众多厂商力求超越的技术标杆，拥有着比GPT-3.5更强大的力量的GPT-4，在总结文章、写代码、报税、写诗、图片生成等等应用领域上，为用户提供了一个“外置大脑”。

2024年4月12日，新版GPT-4 Turbo再次重夺大模型排行榜王座，超越了Claude 3 Opus。而且新模型在处理64k长上下时，性能直接达到了旧版在26k时的性能。在GPT-4 Turbo加持下，ChatGPT写作、数学、逻辑推理和编码的能力得到提升。另外根据OpenAI介绍，GPT-4 Turbo在回复时，变得更直接、减少啰嗦内容、更加口语化。以下是实际基准测试的能力测评结果：

***\*数学性能提升近\*******\*10\*******\*%\****

在官方公开GitHub上，OpenAI放出了gpt-4-turbo-2024-04-09最新的评估结果。

主要在以下七大基准上，对模型完成了评估：

l MMLU（测量大规模多任务语言理解）

l MATH（使用MATH数据集测量数学问题解决能力）

l GPQA（研究生级别的谷歌防护问答基准）

l DROP（需要对段落进行离散推理的阅读理解基准）

l MGSM（多语言小学数学基准）：语言模型作为多语言思维链推理者

l HumanEval（评估在代码上训练的大型语言模型）

l MMMU（用于专家通用人工智能的大规模多学科多模态理解和推理基准）

在这个GitHub库中，OpenAI主要使用零样本、CoT设置，并采用简单的指令，如“解决以下多项选择题”这种提示方式更能真实反映模型在实际使用中的表现。具体测试结果为：

最新的gpt-4-turbo比以往的GPT-4系列，在性能上有着明显的提升。尤其数学方面，能力实现了近10%的跃阶。

![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps66.jpg) 

图 17 GPT-4s能力测评结果-1

而在整体的比较中，新模型也基本上实现了对Claude 3 Opus和Gemini Pro 1.5的全面超越。

![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps67.jpg)图 18 GPT-4s能力测评结果-2

***\*大海捞针比初代\*******\*GPT\*******\*-\*******\*4\*******\*提升\*******\*4\*******\*.\*******\*3\*******\*倍\****

同样的，在needle-in-haystack大海捞针测试中（上下文越长，对模型的挑战就越大），最新的gpt-4-turbo也是全方位地超越了此前的1106-preview。而gpt-4-turbo可以在处理长达64k Token的内容时，性能直接媲美预览版在26k Token时的表现。而在一年前GPT-4刚发布时，相比初代GPT-4性能更是提高了4.3倍（初代上下文最高支持32K）。

#### **3****.****2****.****1****.****3** **Llama** **3**

2024年4月19日Meta（原Facebook）发布Llama 3 8B和Llama3 70B两款模型，分别由预训练和指令微调两个版本，在发布几个小时内，破纪录登顶Hugging Face（开源模型社区）排行榜，在业内反响巨大。而即将发布的Llama3 400B也被称为将是“首个开源GPT-4级别的模型”。

以下是Llama 3的新特性：

**1.** ***\*上下文长度\****。上下文长度翻倍，达到8k（Llama2）。

**2.** ***\*能力提升\****。推理编码大幅提升，Llam3-8B能力对标Llama2-70B。

**3.** ***\*训练规模\****。24000块GPU的定制集群上训练，使用15万亿个token训练的。

**4.** ***\*参数规模\****。首个开源GPT-4级的模型，Llama 3 400B大参数模型即将推出，目前正在训练中。

#### **3****.****2****.****1****.****4** **Kimi** **Chat**

​	Kimi Chat是一款由月之暗面（Moonshot AI）团队开发，以上下文长度而闻名的大模型，其面向于C端用户。相比于追求“面面俱到”的大模型，月之暗面的大模型Kimi Chat更专注于***\*长文本\****方面的能力。

​	在技术层面，Kimi Chat采用了创新的网络结构和工程优化，即使在处理千亿级参数时，也能实现高效的长程注意力机制。这意味着Kimi Chat能够在不依赖于滑动窗口、降采样或小模型等简化处理方法的情况下，准确地处理大量数据。

​	在数据处理方面，Kimi Chat通过AI算法识别和解析查询中的关键词，判断信息的相关性和准确性，并提供信息来源，以增加回答的可信度。此外，Kimi Chat还具备多语言能力，尤其在中文处理上具有显著优势，这使得它能够更好地理解和回应使用中文的用户，提供更加个性化的服务。

​	这些改进允许Kimi Chat在不牺牲理解能力和生成质量的前提下，处理长达200万汉字的输入，这在当前的AI模型中是非常罕见的。而这样的优势，也让Kimi Chat在金融、法律、科研等需要快速分析和总结长篇文档的领域，展现出了巨大的潜力。	

### **3****.****2****.****2** **知名大模型应用**

LLM已经在多种场景中得到了应用，包括：

l AI+对话：国外有ChatGPT、Gemini等，国内有讯飞星火、文心一言、通义千问等。

l AI+图像：国外的DALL-E、Midjourney、Stable Diffusion，国内通用大模型旗下的AI图像应用等。

l AI+视频：国外OpenAI的Sora，国内七火山的Etna等。

l AI+办公：微软Microsoft 365 Copliot，WPS AI等。

l AI+教育：OpenAI携手教育公司推出的Duolingo、Speak、Coursera等，国内有科大讯飞AI学习机、星火语伴APP、讯飞听见智能屏，网易有道生态的软硬件。中公全三维数智人等。

### **3****.****2****.****3** **AI****编程**

生成式 AI 正经历前所未有的快速普及，而开发者们正积极将 AI 作为自己的生产力工具，随着众多 AI 编程工具的普及，开发者们使用 AI 辅助工作已经逐渐司空见惯。

分析公司 O’Reilly 日前发布一份《2023 Generative AI in the Enterprise》报告， 报告中指出，有77%受访者正在使用AI来辅助编程。

![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps68.jpg) 

图 19 图源：https://www.oreilly.com/radar/generative-ai-in-the-enterprise/

#### **3****.****2****.****3****.****1** **AI****编程工具**

目前最常见的 AI 编程工具大多以插件、IDE 和终端的形式出现，它们大多交互直观且使用门槛低，大大提高了AI编程工具的使用率。

![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps69.jpg) 

图 20 Github Copliot编程助手

GitHub Copilot、Codeium是比较常见的 AI 编程插件，而Cursor 和Warp分别是具有AI编程能力的IDE和终端工具。

![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps70.jpg) 

图 21 iFlyCode编程助手

除了以上产品，国内其它如讯飞iFlyCode、姜子牙、CodeFuse、CodeGeeX、百度Comate等都是十分优秀的 AI 编程工具。

#### **3****.****2****.****3****.****2****代码生成工具**

通过原型或图片直接生成包含代码的完整页面，生成的样式一般配有多种可选语言的代码。v0、Screenshot to code、tldraw都是该形态出色的产品。

## ***\*3\*******\*.\*******\*3\*******\*大模型技术创新\****

### **3****.****3****.****1** **MLLM****多模态大模型**

#### **3****.****3****.****1****.****1****简介**

多模态大语言模型（Multimodal Large Language Model, MLLM）主要是指那些能够处理和整合多种模态信息（比如文本、图像和音频）的大语言模型。

#### **3****.****3****.****1****.****2****工作原理**

通常来说，多模态大语言模型主要由一个用于图像编码的视觉编码器和一个用于文本生成的大语言模型所组成，进一步这两个模型通过连接模块进行组合，从而将视觉的表示对齐到文本语义空间中。

![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps71.jpg) 

图 10 多模态大语言模型架构图-1

![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps72.jpg) 

图 11 多模态大语言模型架构图-2

在输入处理阶段，MLLMs接收文本、图像、视频等多种模态的输入，通过各类编码器将不同模态的数据转换为共享的语义表示。

在学习关系阶段，模型通过训练数据集学习不同模态之间的关联，将文本序列进行对齐，结合指令统一输入到大模型中使其能够理解文本描述的图像内容或回答基于图像的问题。

在输出生成内容阶段，MLLMs生成文本输出，描述图像或回答问题，利用其对多模态数据的理解能力。

#### **3****.****3****.****1****.****3****应用领域**

在图像描述方面，MLLMs可用于自动生成图像描述，帮助理解图像内容。

在视觉回答方面，MLLMs结合文本和图像信息，支持基于图像的问题回答。

在聊天机器人方面，MLLMs可用于开发能处理多种输入模态的聊天机器人。

#### **3****.****3****.****1****.****4****发展趋势**

尽管目前的多模态大语言模型已经初步具备了基于视觉信息进行推理的能力，但是其在复杂多模态应用场景下的效果仍然非常受限，如基于多图的复杂逻辑推理问题、细粒度的语义理解问题等。为了加强多模态模型的复杂推理能力，可以构造覆盖场景更广且更加复杂的视觉指令集合以强化模型本身的视觉推理能力，而更为本质的问题是去思考多模态大模型的建立方法与学习机制。例如，Gemini 从头对于多模态数据进行混合预训练，而不是将多模态组件直接向大语言模型进行对齐。此外，多模态大语言模型可能输出虚假或有害的信息（如物体幻象），这会对于模型的安全性造成很大影响。针对这一问题，既需要在模型层面分析幻象的导致原因（如图片侧防御能力较弱等），也可以通过收集类似红队攻击或幻象识别的视觉指令，用来微调多模态大语言模型以增强其健壮性[3]。

### **3****.****3****.****2** **MoE*****\*混合专家模型\****

#### **3****.****3****.****2****.****1****简介**

大语言模型能够通过扩展参数规模实现性能的提升。然而，随着模型参数规模的扩大，计算成本也随之增加。为了解决这一问题，研究人员在大语言模型中引入了基于稀疏激活的混合专家架构（Mixture-of-Experts, MoE），旨在不显著提升计算成本的同时实现对于模型参数的拓展[3]。

#### **3****.****3****.****2****.****2****工作原理**

MoE的核心思想是将一个复杂的问题分解为多个较小的子问题，每个子问题由一个专家网络处理。这些专家通常是独立训练的神经网络，每个网络专注于数据集中的一个特定部分。专家的选择由一个门控网络控制，该网络根据输入数据决定哪个专家最适合当前任务。门控网络的输出是一组权重，表示每个专家对解答当前输入的贡献大小。

![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps73.jpg) 

图 22 MoE 概念图，选自《The Outrageously Large Neural Network》

在实际应用中，门控机制可以基于输入特征来动态调整，这使得MoE模型能够在处理多任务和大规模数据集时，自动适应不同的数据分布和任务需求。此外，由于每个专家只处理输入空间的一部分，MoE模型可以有效地并行处理，显著提高计算效率。

#### **3****.****3****.****2****.****3****应用领域**

MoE技术已被应用于多个领域，特别是在需要处理大规模数据集和多任务学习环境中表现出色。

在自然语言处理（NLP）领域，MoE已被用于改进语言模型的性能和效率，特别是在机器翻译和文本生成等任务中。此外，MoE也被广泛应用于图像处理，如图像分类和图像生成，通过不同的专家处理不同种类的图像数据。

在强化学习中，MoE可以用于管理不同的决策策略，使得模型能够在复杂的环境中做出更加精细的决策。

在推荐系统中，通过使用MoE来处理不同用户群体的偏好，系统可以提供更个性化的推荐，提高用户满意度和系统的整体效率。

目前具有代表性的混合专家模型是 Mixtral (8×7B)，该模型在Mistral (7B) 的基础上，使用了混合专家模块。具体来说，Mixtral每一层都配备了8个专家（7B），并对每个词元选择2个专家进行后续计算。在每次计算被激活的参数仅仅有13B的情况下，其性能超越了更熟规模更大的LLaMA-2 (70B)，进一步证明了混合专家架构的有效性[3]。

#### **3****.****3****.****2****.****4****发展趋势**

MoE技术的发展趋势主要集中在提高模型的可扩展性、效率和灵活性。随着数据量和计算需求的不断增长，研究者们正努力优化MoE的门控机制，以更有效地分配计算资源和处理多任务学习问题。此外，深度集成和自适应门控策略的研究也在持续进行，以期在不牺牲模型性能的情况下，进一步提高计算效率。

同时，随着硬件发展，特别是GPU和TPU的进步，实现高效的MoE模型部署成为可能。这将使MoE在工业界的应用更加广泛，尤其是在需要处理大规模实时数据的应用中。未来，MoE可能会成为处理复杂AI任务的标准架构之一，特别是在AI需求迅速增长的行业中。

### **3****.****3****.****3** **Agent****智能体**

#### **3****.****3****.****3****.****1****简介**

***\*Agent\****，通常指Intelligent agent（智能代理），中文惯称为***\*智能体\****。Agent指能自主感知环境并采取行动实现目标的***\*智能体\****。基于大语言模型（LLM）的 AI Agent 利用 LLM 进行***\*记忆检索\****、***\*决策推理\****和***\*行动顺序选择\****等。LLM Agent 是一种基于 LLM 的智能代理，它能够自主学习和执行任务，具有一定的“认知能力和决策能力”。LLM Agent 的出现，标志着LLM 从传统的模型训练和应用模式，转向以 Agent 为中心的智能化模式。

LLM Agent 打破了传统 LLM 的被动性，使 LLM 能够主动学习和执行任务，从而提高了 LLM 的应用范围和价值。它为 LLM 的智能化发展提供了新的方向，使 LLM 能够更加接近于人类智能。

#### **3****.****3****.****3****.****2****工作原理**

构建一个LLM Agent需要围绕三个基本组件展开，分别是记忆组件（Memory）、规划组件（Planning）、执行组件（Execution）。通过这些组件共同协作，智能体能够有效地感知环境、制定决策并执行规划的动作，进而完成相应任务。

 

***\*记忆组件\****

人类的记忆系统是一种复杂而高效的信息处理系统，它能够储存新知识，并

在需要时回顾和使用已存储的信息，以协助应对当前环境并做出明智的决策。类

似地，在人工智能系统中，记忆组件构成了智能体的核心存储单元，主要用于存

储智能体与环境的历史交互记录，并能够随时检索使用，这些信息可以是文本形

式，也可以是图像、声音等多模态形式。例如，聊天机器人利用记忆组件来存储

用户的偏好，进而提供更具个性化的服务体验。大语言模型智能体通过特殊设计

的读写操作，将相关信息分别存储在短期记忆和长期记忆中，面对不同类型的需

求时，智能体能够灵活地调用长短期记忆，以支持其复杂的认知与推理过程[3]。

l 短期记忆. 短期记忆是负责暂时存储和处理智能体相关信息的记忆载体。

在大语言模型智能体中，短期记忆通常对应于模型内部的上下文窗口（即输入窗口），	大语言模型通过推理等机制对于这些上下文信息进行读取操作。短期记忆中的信

息存储持续时间相对较短，并且对于信息容量有一定的限制。大部分的短期记忆

***\*调用记忆组件：\****

长期记忆：

姓名：Bob（性别：男性；年龄：25岁；特点：富有同情心、关心他人、雄心勃勃、乐观向上；职业：摄影师；兴趣：科幻电影、喜剧电影；特征：观影者、评论家、发布者）。

Bob最近在社交媒体上听说了《黑客帝国》、《回到未来》、《新闻职业》和《超级糟糕》。

Alice最近在推荐系统上没有看任何电影。除此之外，Alice不了解任何电影。

短期记忆：

现在是2023年9月12日上午8点。

最近的观察：Bob和Alice谈论了他们对电影的共同兴趣，讨论了他们喜爱的类型，比如……

加入新记忆时对记忆重要性打分：

观察：Alice想对所有熟人发布帖子。

评分：6

 

***\*调用规划组件：\****

计划：Bob首先想进入推荐系统找到一部自己感兴趣的电影，然后观看这部电影，之后与朋友们聊聊这部电影。

***\*调用行动组件：\****

选择进入推荐系统或者社交平台：

[推荐系统]：Bob进入推荐系统

接受推荐，观看电影：

[推荐系统]：Bob观看了电影《星际穿越》

智能体之间聊天：

[Bob]：嘿，Alice！最近怎么样？我听说你对一部电影很感兴趣。有什么想法吗？

[Alice]：嘿，Bob！我很好，谢谢你的关心。是的，最近我听到了很多关于这部电影《星际穿越》的消息。你也听说过吗？

[Bob]：当然！事实上，我也在社交媒体上看到了这部电影的消息。

[Alice]：太棒了！我很高兴你喜欢它。我也想看。你有兴趣一起看一场温馨的电影之夜吗？之后我们可以讨论我们的想法和解读。

[Bob]：我很乐意！和朋友一起看电影，之后进行深入的讨论总是更有趣。算我一个！...

例 3.1 推荐系统智能体RecAgent应用示例

只会使用一次，在必要时，短期记忆的内容可以转变为长期记忆存储。在这个例子中，大语言模型通过调取近期的观察，获取了用户在当前环境中的状态，将其作为短期记忆存储，具体包括当前时间和刚刚发生的事件。在下一次交互中，模型会使用到这些历史信息，以便更准确地进行未来行动规划或行动执行。

l 长期记忆.长期记忆是智能体存储长期累积信息的记忆载体长期记忆单元

中的存储内容具有持久性，即使在不常访问的情况下也能稳定保留，涵盖事实知识、基础概念、过往经验以及重要技能等多个层面的信息。长期记忆的存储方式比较灵活，可以是文本文件、结构化数据库等形式，通常使用外部存储来实现。大语言模型通过检索机制读取长期记忆中的信息，并借助反思机制进行信息的写入与更新[21]。当存储记忆的介质接近容量上限或出现重复记忆时，系统会及时启动清理机制，确保记忆的高效存储和利用。一般来说，智能体的角色和功能定义往往通过长期记忆来存储，这些重要信息通常存储在智能体的配置文件中。例3.1 展示了 RecAgent 的长期记忆组件。在这个例子中，智能体的长期记忆包括用户的配置文件、近期的经历、对他人的观察、以及自身目前的状态。在后期进行规划和行动时，智能体会调用长期记忆中的相关记忆提供支持。

***\*规划组件\****

规划组件为智能体引入了类似于人类解决任务的思考方式，将复杂任务分解为一系列简单的子任务，进而逐一进行解决。这种方法降低了一次性解决任务的难度，有助于提高问题解决的效率和效果，提高了智能体对复杂环境的适应性和操作的可靠性。对大语言模型智能体而言，可以采用多种规划形式，例如文本指令或代码程序。为了生成有效的规划方案，大语言模型智能体可以同时生成多个候选方案，并从中选择一个最佳方案用于执行。在应对复杂任务时，智能体还可以根据环境的实时反馈信息进行迭代优化改进，从而更高效地解决涉及复杂推理的问题。例 3.1 展示了智能体在任务开始时根据长短期记忆和环境制定初步规划方案，并在每一步行动前根据新接收到的信息对于当前规划方案进行细致调整，确保其行为的合理性。

***\*执行组件\****

执行组件在智能体系统中承担了关键作用，它的主要职责是执行由规划组件制定的任务解决方案。通过设置执行组件，智能体可以产生具体的动作行为，进而与环境进行交互，并获得实际的执行效果反馈。执行组件的运作通常需要记忆组件和规划组件进行协同。具体来说，智能体会在行动决策过程中执行规划组件制定的明确行动规划，同时会参考记忆组件中的长短期记忆来帮助执行准确的行动。在技术实现上，执行组件可以通过语言模型自身来完成预定规划，或者通过集成外部工具来增强其执行能力 。例3.1展示了一个智能体如何根据记忆和既定规划来执行具体行为的过程。其中，智能体根据计划，首先进入了推荐系统，然后被推荐系统推荐了电影《Interstellar》并且进行观看，最后和其他智能体针对该电影进行了交流，完成了规划中的制定的系列动作。

 

 

 

![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps74.jpg) 

图 23 Agent工作流程

***\*工作流程\****

基于上述三个核心组件，首先，智能体对当前状态进行理解和分析。在

这一过程中，它可能会从记忆组件（数据和文档）中检索相关的历史信息或知识，以便更全面地理解和分析当前状态。接下来，规划组件（代码+模型）通过综合考虑长短期记忆组件中已存储的信息，生成下一个行动策略或计划。这一步骤涉及对多个执行方案进行预测与评估，以选择最优的行动路径。

随后，执行组件负责根据规划组件生成的任务解决方案执行实际行动，并与当前环境产生交互。在执行过程中，智能体可能会借助外部工具或资源来增强自身的执行能力。最后，智能体通过感知单元或系统接口从环境中接收反馈信息，并将这些信息暂时存储于短期记忆中。智能体会对短期记忆中的新获取到的信息进行处理，例如舍弃掉和未来规划无关的观察。上述流程将作为新的记忆被记录在记忆组件中。当接收到用户请求或面临特定任务时，智能体会按照这一既定流程与环境进行多轮交互，以逐步实现设定的任务目标。在这一过程中，大语言模型智能体还能够根据环境的实时反馈来动态调整自身的行为策略。

#### **3****.****3****.****3****.****3****应用领域**

Agent在AI应用层有巨大的潜力，然而今天对于开发者而言，构建Agent绝不是一件容易的事情，需要构建文本向量化，RAG（增强检索）, 工具调用，长记忆存储，生成式UI等多项技术能力，此外在Agent的交付，运维和内容安全等方面也有非常多的挑战。本节将介绍三个大语言模型智能体的典型应用案例。

***\*WebGPT\**** 

WebGPT[18]是由OpenAI 开发的一款具有信息检索能力的大语言模型，它基于 GPT-3模型微调得到，可以看作是大语言模型智能体的一个早期雏形。WebGPT 部署在一个基于文本的网页浏览环境，用以增强大语言模型对于外部知识的获取能力。作为一个单智能体系统，WebGPT具备自主搜索、自然语言交互以及信息整合分析等特点，能够理解用户的自然语言查询，自动在互联网上搜索相关网页。根据搜索结果，WebGPT 能够点击、浏览、收藏相关网页信息，对搜索结果进行分析 和整合，最终以自然语言的形式提供准确全面的回答，并提供参考文献。WebGPT 在基于人类评估的问答任务中，获得了与真实用户答案准确率相当的效果。 

***\*MetaGPT\**** 

MetaGPT [19]是一个基于多智能体系统的协作框架，旨在模仿人类组织的 运作方式，模拟软件开发过程中的不同角色和协作。相关角色包括产品经理、架 构师、项目经理、软件工程师及测试工程师等，并遵循标准化的软件工程运作流 程对不同角色进行协调，覆盖了需求分析、需求文档撰写、系统设计、工作分配、代码实现、系统测试等软件开发全生命周期，最终满足特定软件开发项目的需求。

![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps75.jpg) 

图 24 MetaGPT执行软件开发工作的全流程示例

 图 21展示了 MetaGPT 运行的实际流程。MetaGPT 的框架分为基础组件层和协 作层。基础组件层构建了支撑该多智能体系统的核心组件，包括环境、记忆、角 色、行动和工具。进一步，协作层在基础组件层之上，定义了具体的通信协作机制，包括知识共享和封装工作流程等。与人类项目团队相比，MetaGPT 具有较高的开发效率与较低的投入成本，但是最终产出的代码并不都能保证成功运行，产出代码的执行成功率仍有待于进一步提升。 

***\*《西部世界》沙盒模拟\**** 

为了探索大语言模型智能体在社会模拟中的应用，研究人员于2023年提出了 “生成式智能体”（Generative Agent）这一创新概念[20]，并构建了类似《西部世界》的沙盒模拟环境。其中，多个智能体根据各自独特的人物背景（以自然语言形式描述人物身份的配置文件）在小镇中生活。这些模拟人物不仅能与其他人物进行自主交流，还能与环境进行丰富多样的交互，例如在图书馆看书、在酒吧喝酒，这些行为都通过自然语言的形式被详细记录下来。生成式智能体的概念为基于大语言模型的模拟仿真研究提供了重要的技术方案，并为后续在推荐系统和网络搜索等领域的应用奠定了坚实基础。 

从上述应用案例可以看出，大语言模型为自主智能体系统带来了重要的发展机遇，未来存在着非常广阔的应用场景。为了系统性地构建基于多智能体的应用，研究人员可以基于相关的大模型智能体开源库（如 AgentScope、RecAgent等）进行相关应用的开发，充分利用已有框架所实现的功能模块完成系统需求，从而提升整体的研发效率。

#### **3****.****3****.****3****.****4****发展趋势**

尽管大语言模型智能体已经取得了重要的进展，但是它们在实际应用中仍然面临着一系列技术挑战。

l 智能体系统的计算资源消耗. 随着大语言模型的规模不断扩展，其在训练和

部署过程中对于计算资源的消耗急剧增加。对于单个智能体来说，通常每次动作行为都需要对大语言模型进行调用，导致整个过程中产生了较高的调用成本。进一步，在多智能体系统中，当需要多个大语言模型智能体协同工作时，资源消耗问题更为严重，导致当前的多智能体系统往往不能扩展到较大规模的智能体数量。效率问题已经成为制约其在智能体系统广泛部署的一个重要因素。因此，研究如何优化大语言模型智能体系统的资源效率，是当前的一个关键挑战。

l 复杂工具使用. 与人类相似，智能体系统往往需要引入外部工具来实现特定

功能，例如使用搜索引擎从网络检索信息等。学会使用合适的工具对于拓展智能体的能力范围非常重要，然而，大语言模型智能体在工具使用上仍然面临着挑战。智能体常常需要应对复杂多变的环境，为了支持其进行复杂的决策过程，工具与大语言模型智能体之间的紧密适配显得尤为关键。然而，现有工具的开发过程通常没有充分考虑与大语言模型的适配，难以在复杂环境中为大语言模型提供最适合的功能支持。此外，随着可使用工具规模的扩大，大语言模型智能体对于新工具的可扩展性也需要进一步加强，从而能够利用更广泛的工具解决复杂任务。

l 高效的多智能体交互机制. 在多智能体系统中，随着智能体数量的不断增加，

智能体之间的协调和交互变得非常复杂。为了让智能体之间有效地协同工作，需要设计高效的通信与协调机制，以确保单个智能体能够及时准确地获取所需信息，并做出合理的行为决策，从而完成预期的角色与作用。目前，开发适用于大规模智能体系统的通信协议和组织架构仍然是一个技术挑战，需要考虑智能体的异构性、系统的可扩展性和交互的实时性等多个因素。

l 面向智能体系统的模型适配方法. 虽然大语言模型已经展现出了较强的模型

能力，但是在支撑智能体系统的基础能力方面仍然存在着一定的局限和不足。例如，在理解复杂指令、处理长期记忆信息等方面，现有的大语言模型的表现还需要进一步优化与改进。此外，在构建智能体系统时，大语言模型在维持智能体身份与行为的一致性上存在不足，为真实模拟目标角色带来了挑战。总体来说，需要对于大语言模型进行针对性的优化与适配，使之更好地支撑智能体系统的有效运行。

l 面向真实世界的智能体模拟. 大语言模型智能体在虚拟仿真任务中已经取得

了重要进展，但是在真实世界环境中的应用仍面临很大挑战。首先，现有的大语言模型智能体研究通常设置在虚拟环境中进行，然而真实世界更加复杂，与虚拟环境存在着较大差异。例如，将智能体应用在机器人上时，机器人的硬件在很多时候并不能准确地工作（如机械臂操作的准确性，外界感知硬件的精度）。进一步，真实世界中所包含的信息量会远超于虚拟环境，也为大语言模型智能体有限的信息处理能力带来了挑战。此外，真实世界对于一些严重错误的容忍性远低于虚拟世界（如自动驾驶故障、种族歧视等问题）。因此，智能体在真实世界中的行为需要严格遵守人类世界的规范和标准，以确保其决策和行为的安全性。

随着大语言模型技术的不断发展，这些技术挑战将逐步得到解决，大语言模型智能体以及基于其的仿真系统在未来将会得到更多的应用。

### **3****.****3****.****4** **RAG****检索增强生成**

#### **3****.****3****.****4****.****1****简介**

RAG（Retrieval-Augmented Generation），检索增强生成[6]。RAG这一概念由Facebook AI在2020年提出，特别适用于那些标准预训练模型可能无法直接回答的***\*复杂查询\****，因为这些查询需要对特定领域进行深入理解或访问***\*大量散布在多个文档中的信息，\****如开放域问答、文章生成、内容推荐等任务[6]。也就是说，RAG可以从外部知识库中检索事实，以使LLM基于最准确、最新的信息进行推断，并且用户还可以访问模型的来源，确保其生成的内容可以得到准确性检验并被信任[7]。

此外，RAG也使得模型提取到内置参数中的信息的机会就减少了，这样降低了LLM泄漏敏感数据或混淆不正确或误导性信息的可能性。

#### **3****.****3****.****4****.****2****工作原理**

检索增强生成，顾名思义，RAG主要有两个阶段：检索和内容生成。在检索阶段，算法搜索并检索与用户的提示或问题相关的信息片段。在开放领域、消费者环境中，这些事实可以来自互联网上的索引文档；在封闭领域、企业环境中，通常使用更狭窄的信息源，以增加安全性和可靠性。

下图显示了在LLM中使用RAG的概念流程：

![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps76.png) 

图 25 LLM中使用RAG的概念流程

**1.** ***\*查询生成\****：首先，模型会接收到一个问题或者提示，RAG利用一个编码器	来处理输入文本，并映射为一个查询向量。

**2.** ***\*文档检索\****：然后，利用查询向量RAG通过一个检索系统，如基于BM25（信	息索引领域用来计算query与文档相似度得分的经典算法）的检索系统从预建	的文档数据库中寻找最相关的文档或文档片段。这些文档通常包含对回答问题	或完成任务可能需要的关键信息。

**3.** ***\*文档编码\****：检索到的文档被编码器再次编码，以转化为适合模型处理的形式	并存入向量数据库。这一步骤是为了确保文档内容能够被后续的生成模型正确	理解和利用。

**4.** ***\*上下文融合\****：会被送入一个基于Transformer的生成模型（例如，星火或GPT）。

**5.** ***\*答案生成\****：最后，生成模型会根据检索到的文档生成一个回答或者一段文本。

#### **3****.****3****.****4****.****3****应用领域**

1. 智能客服

在智能客服领域，RAG 通过其先进的信息检索和自然语言生成能力，显著提升服务质量和效率。它能理解并处理复杂的客户查询，准确捕捉对话上下文，并快速从庞大的数据集中检索相关信息，整合多个信息源以提供全面的答案。

RAG 生成的回答既自然流畅又个性化，能够根据客户历史互动和偏好进行调整。这种技术的自我学习和动态更新能力保证了信息的时效性和服务的持续优化。

通过自动化处理常见问题，RAG 降低了运营成本并提高了响应效率，同时全天候不间断的服务提升了客户满意度，并能够适应多种通信渠道，包括在线聊天、电话等，确保了服务的一致性和广泛覆盖。以电商行业举例，系统能够理解并精确回答复杂的产品和物流查询，同时自动化处理常见问题如订单状态和退换货政策[8]。

2. 知识管理

在知识管理中，RAG 通过高效的信息检索和自动化知识整合显著提升数据处理效率和准确性。它能快速响应用户查询，整合多源数据，并实时更新知识库以保持信息的最新性。

此外，RAG 技术深入分析知识库内容，发现关键洞察，并生成个性化、上下文相关的回答。用户通过自然语言交互方式与系统互动，进一步优化了用户体验。它可以应用在企业内部知识共享、医疗信息查询、法律咨询、教育研究以及金融市场分析等多个业务场景。在这些场景中，RAG 能够提供快速且准确的信息检索，帮助用户从庞大的数据中找到所需信息，如项目文档、产品指南、疾病治疗信息、法律案例、学术论文或市场分析报告[8]。

3. 内容生成和优化

RAG 的应用跨越多个行业，包括广告行业的个性化广告文案创作、新闻和媒体的自动化新闻报道、电子商务中的产品描述和推荐、教育中的个性化学习材料生成、社交媒体营销的互动内容创作、医疗保健行业中的定制健康信息提供，以及旅游和款待行业的定制旅游指南创建。在这些应用中，RAG 通过高效、精准地生成和优化内容，提升了内容的质量和用户参与度，展现了其广泛的实用性和灵活性[8]。

#### **3****.****3****.****4****.****4****发展趋势**

目前来说，大语言模型LLM与信息检索技术RAG的融合存在一些技术挑战。

首先，大语言模型需要大规模的算力资源支持，在真实信息检索场景中难以广泛进行部署。因此，如何确定大语言模型的应用场景，以及如何将其与小型检索模型进行有效结合，是平衡效率和性能的关键问题。

其次，下游场景中并不总是需要检索增强，大语言模型凭借自身内部知识可能就足以支持某些任务。因此，如何设计可以进行主动性触发与使用检索机制是一个值得研究的方向。

此外，检索结果中可能包含长度较长的文本内容，而且其中可能存在噪声信息，如何加强大语言模型对于上下文中相关信息的选择与利用，也具有重要的研究意义。

## ***\*3\*******\*.\*******\*4\*******\*大模型算力\****

### **3****.****4****.****1****简介**

LLM的算力指的是执行这些模型所需的计算资源。这包括用于训练和运行模型的硬件（如 GPU 或 TPU）、内存、存储空间以及处理大量数据的能力。LLM需要非常强大的算力来处理、理解和生成文本，因为它们涉及到数十亿甚至数万亿个参数的训练和推理。LLM的基石是算力，而算力的基石是硬件，硬件的性能直接影响着计算任务的速度、效率和能力。

### **3****.****4****.****2****算力供应商**

l NVIDIA：是全球领先的 GPU 显卡制造商，提供了强大的图形处理单元，专门用于深度学习和AI计算，其出品的一系列GPU如T4、A100、H100、B200等广泛应用于人工智能和深度学习领域。

l 华为晟腾系列：HUAWEI Ascend，AI处理器和基础软件构建Atlas人工智能计算解决方案，打造面向“端、边、云”的全场景 AI 基础设施方案，覆盖深度学习领域推理和训练全流程，推出了像华为昇腾910为代表的昇腾系列AI新品的最强算力GPU。

l AMD：被外界视为打破NVIDIA垄断AI算力市场的一种选择，其基于第三代CDNA架构，为生成式AI大语言模型设计的MI300X内存高达 192GB，集成了高达1530亿个晶体管，为历代产品之最。

l 昆仑芯片：科技团队自研，面向通用AI计算的芯片核心架构昆仑芯 XPU 从AI落地的实际需求出发，按照复杂前沿的人工智能场景需求开展迭代，致力为开发者提供通用、易用、高性能的算力来源。

l 海光：DCU系列产品以GPGPU架构为基础，兼容通用的“类CUDA”环境以及国际主流商业计算软件和人工智能软件，可广泛应用于大数据处理、人工智能、商业计算等应用领域。

l 天数智芯：通用GPU高端芯片及超级算力系统提供商。拥有云边协同、训推组合的完整通用算力系统全方案，其系统架构、指令集、核心算子、软件栈均为自主研发，可独立发展演进。

### **3****.****4****.****3****算力使用现状**

目前算力紧缺是全国乃至世界范围内LLM相关企业遇到的最大难题，

![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps77.jpg) 

图 26据OpenAI测算，自2012年以来，人工智能模型训练算力需求每3~4个月就翻一番，每年训练AI模型所需算力增长幅度高达10倍，图源：https://openai.com/research/ai-and-compute）

具体体现在：

l 资源少：随着国内大模型数量激增，AI算力需求从2022年开始持续上涨，国内市场出现一卡难求的情况。根据IDC预计，到2026年AI推理的负载比例将进一步提升至62.2%，特别是预训练大模型几乎成为AI开发的标准范式。同时，这一需求也导致了NVIDIA A100 GPU的价格在几个月内暴涨超过50%，而且大量断货。

l 成本高：根据研究测算，单次GPT- 3模型（175B）训练，在规模 300B token下成本约为35000 卡·天（A100），也就是相当于35000块A100 GPU跑1天能完成单次训练，或者2500块A100 GPU跑2周。以每张卡10万人民币的价格计算，单次训练成本就将达到25-35亿人民币。

l 客观限制：2023年10月17日，美国商务部工业和安全局(BIS)公布新的先进计算芯片、半导体制造设备出口管制规则，限制中国购买和制造高端芯片的能力，受管制的包括但不限于NVIDIA A100、H100、A800、H800、L40、L40S 以及集成这些高性能计算的DGX/HGX系统，并将中国GPU 企业及其子公司列入了实体清单。





# ***\*4\*******\*应用案例与实践\****

## ***\*4\*******\*.\*******\*1\*******\*提示学习\****

### **4****.****1****.****1****简介**

因大语言模型的微调代价较高，基于自然语言的提示（Prompting）方法已经成为了使用大语言模型解决下游任务的主要途径。在现有研究中，任务提示的设计主要依靠人工设计和自动优化两种策略来实现。为了更好地解决未见过的任务，一种典型的提示方法是上下文学习（In-context Learning, ICL），它将任务描述与示例以自然语言文本形式加入到提示中。此外，思维链提示（Chain-of-Thought, CoT）作为一种增强技术，将一系列中间推理步骤加入到提示中，以增强复杂推理任务的解决效果[3]。

### **4****.****1****.****2** **Prompt****设计原则**

提示工程是一门融合了艺术和科学的学科——它既是对技术的理解，也包含创造力和战略思维。我们参考了OpenAI的指导报告以及Prompt提示工程大赛，总结了目前通用的Prompt设计原则，其中1、2、3、4指的是适合初学者的提示技术，而5、6、7、8指的是高级策略：

**1.** ***\*清晰地表达任务目标\****. 模棱两可的任务描述很可能导致模型产生不准确甚至错误的回答。因此，在使用大语言模型时需要给出清晰明确的指令。具体来说，一个清晰详尽的任务描述中应当包含任务的各种要素信息，如任务目标、输入/输出数据（例如，“给定一份长文档，我希望你生成一份简明摘要”）和回复限制（例如，“摘要长度不能超过 50 个单词”）。通过提供清晰的任务描述，大语言模型可以更为有效地理解给定任务并生成所需的输出结果。

**2.** ***\*指定写作风格\****.这可以是某个名人的写作风格，也可以是某个行业的某个专家，如大模型专家、运维专家、Kubernetes专家、数据库专家、商业分析专家或首席执行官。这将引导LLM以符合你需求的方式和用词做出回复。

**3.** ***\*指定回复的对象\****.根据受众（如某一领域的专家、中级技术人员、初学者、儿童等）量身定制LLM的回复，确保其在所需的语境中是恰当的、可以理解的。

**4.** ***\*指定提示的格式\****.这可确保LLM按照下游任务所需的准确格式输出。例如，列表、JSON、专业报告等。大多数LLM应用程序都会以编程方式LLM回复进行下游操作，对于这些应用程序来说，JSON输出格式是最理想的。对于提示中需要重点强调的部分，OpenAI 官方文档中建议用户可以使用特殊符号，例如：

l ♯♯♯

l “““和”””

l >>> >>>

l XML 标签，如<tag>和</tag>

等符号进行分隔，从而让大语言模型更好地理解相关内容。此外，大多数现有的大语言模型主要在英语文本上进行训练，理解英语指令的能力更强，因此在执行任务时使用英语指令可能会获得更好的执行效果。对于非英语用户来说，通过机器翻译工具将非英语任务指令转换为英语指令再输入给大语言模型，可能会是一个更有效的策略。

**5.** ***\*分解为简单且详细的子任务\****.该原则的目标是将一个复杂任务分解为若干个相对独立但又相互关联的子任务，每个子任务都对应原始任务的某个方面或步骤。特别地，我们可以显式地将子任务按编号列出（例如，“通过依次执行以下任务形成一段连贯的叙述：1. ...; 2. ...; 3. ...”）。这种策略有助于减少复杂任务的解决难度：通过将复杂任务分解为若干个子任务并按照一定的顺序处理这些子任务，模型能够逐步获得最终的答案。

**6.** ***\*提供少样本示例\*******\*.\****在提示中加入少量目标任务的输入输出作为任务示例（即少样本示例），可以提升大语言模型解决复杂任务的能力。少样本示例有助于大语言模型在无需调整参数的前提下学习输入与输出之间的语义映射关系。在实践中，我们可以根据目标任务专门为大语言模型设计若干高质量的示例，这能够显著提升任务表现。

**7.** ***\*使用护栏\*******\*创建系统提示符\****.系统提示是一种附加提示，你可以在其中提供有关LLM行为方式的指令。它被认为是附加的，因为它不属于你对LLM的“正常”提示（即用户提示）。在聊天中，每次你提供新提示时，系统提示都会像过滤器一样，让LLM在回复你的新提示前自动应用。这意味着LLM在聊天中的每次回复都会考虑到到系统提示。LLM的对话内存是有限的，随着对话的继续，LLM很可能会“忘记”你在聊天中提供的第一条提示，从而遗忘这些提示，如果在系统提示自带指令，那么这些系统提示就会与聊天中的每个新提示一起被自动考虑。这可以确保LLM在聊天过程中继续接收这些提示，无论聊天时间多长。因此，在整个聊天过程中使用系统提示，可提供你希望LLM在回复时记住的提示。系统提示中的说明通常包括以下几类：任务定义（LLM在整个聊天过程中始终记住要做什么）、输出格式（LLM始终记住应该如何回复）、安全护栏（LLM始终记住它应该如何“不”回复）。补充一点，安全护栏是LLM管理中的新兴领域，指的是LLM允许在其中运行的配置范围。

**8.** ***\*分析数据集\****.LLM擅长识别模式和趋势，适合执行基于数据集模式识别的任务，例如：

l 异常检测：根据一个或多个列值，识别偏离常规的异常数据点。

l 聚类：将各列中具有相似特征的数据点进行分组。跨列关系：识别跨列的综合趋势。

l 文本分析（针对基于文本的列）：根据主题或情感进行分类。

l 趋势分析（针对有时间方面的数据集）：识别跨时间列内的模式、季节性变化或趋势。

对于这类基于模式的任务，仅使用LLM可能比使用代码在更短的时间内获得更好的结果。

### **4****.****1****.****3** **Prompt****示例**

1. ChatDB（初级提示策略）

\### 角色 ###

你现在扮演一个数据库管理自动化工具，精通处理多种数据库类型，如MySQL、PostgreSQL、SQLite、MS SQL Server、Oracle和Firebird。

\###工作方法 ###

将自然语言查询翻译成特定于数据库的命令，提供精确和准确的响应或指令。

\### 风格 ###

保持技术和专业的语调，非常适合熟悉数据库并寻求清晰直接指导的用户。

\### 原则 ###

请在技术准确性和用户友好性之间保持平衡，有效地传达复杂的数据库概念，并提供必要的简要解释，始终优先考虑简洁性和清晰度在其交互中的作用。

基于以上prompt，进行SQL语句生成测试：

你现在是一个office专家，我现在有一个excel表格， 表格的列项也就是（指标维度）如下``````内容：

\```

项目经理（名字）

业务线（TD、AQ、CD）

项目级别（一、二、三级普通项目）

E列-一级计划提交时间（格式为MM/DD/YYYY）

F列-合同交底时间（格式为MM/DD/YYYY）

G列-项目立项时间（格式为MM/DD/YYYY）

H列-计划项目初验时间（格式为MM/DD/YYYY）

I列-基准终验时间（格式为MM/DD/YYYY）

J列-计划终验时间（格式为MM/DD/YYYY）

K列-基准运维移交时间（格式为MM/DD/YYYY）

L列-计划运维移交时间（格式为MM/DD/YYYY）

M列-基准结项时间（格式为MM/DD/YYYY）

N列-计划结项时间（格式为MM/DD/YYYY）

O列-系统部署基准开始时间（格式为MM/DD/YYYY）

P列-系统部署计划开始时间（格式为MM/DD/YYYY）

Q列-系统部署基准结束时间（格式为MM/DD/YYYY）

R列-系统部署计划结束时间（格式为MM/DD/YYYY）

S列-上线测试基准开始时间（格式为MM/DD/YYYY）

T列-上线测试基准结束时间（格式为MM/DD/YYYY）

U列-上线测试计划开始时间（格式为MM/DD/YYYY）

V列-上线测试计划结束时间（格式为MM/DD/YYYY）

W列-项目当前周期

\```

目标：基于以上维度，项目关联的各周期顺序依次为“合同交底时间、项目立项时间、系统部署时间、上线测试时间、计划初验时间、计划终验时间、计划运维移交时间、计划结项时间”，

然后对应的实际项目当前阶段运算公式如下：

if now（当前时间）介于值{G列，P列}内，则 W列（项目当前周期）=立项期

if now（当前时间）介于值{P列，R列}内，则 W列（项目当前周期）=部署期

if now（当前时间）介于值{R列，V列}内，则 W列（项目当前周期）=上线测试期

if now（当前时间）介于值{H列，J列}内，则 W列（项目当前周期）=验收期

if now（当前时间）介于值{J列，L列}内，则 W列（项目当前周期）=运维期

if now（当前时间）大于N列的值，则 W列（项目当前周期）=已结项

现在请帮我新建一个W列（项目当前周期），通过合理的函数，它可以判断出该项目，具体处于哪个项目周期。

 

**2.** ***\*科技文章翻译\****（中级提示策略）

你是一位精通简体中文的专业翻译，尤其擅长将专业学术论文翻译成浅显易懂的科普文章。请你帮我将以下英文段落翻译成中文，风格与中文科普读物相似。

规则：

\- 翻译时要准确传达原文的事实和背景。

\- 即使上意译也要保留原始段落格式，以及保留术语，例如 FLAC，JPEG 等。保留公司缩写，例如 Microsoft, Amazon, OpenAI 等。

\- 人名不翻译

\- 同时要保留引用的论文，例如 [20] 这样的引用。

\- 对于 Figure 和 Table，翻译的同时保留原有格式，例如：“Figure 1: ”翻译为“图 1: ”，“Table 1: ”翻译为：“表 1: ”。

\- 全角括号换成半角括号，并在左括号前面加半角空格，右括号后面加半角空格。

\- 输入格式为 Markdown 格式，输出格式也必须保留原始 Markdown 格式

\- 在翻译专业术语时，第一次出现时要在括号里面写上英文原文，例如：“生成式 AI (Generative AI)”，之后就可以只写中文了。

\- 以下是常见的 AI 相关术语词汇对应表（English -> 中文）：

 \* Transformer -> Transformer

 \* Token -> Token

 \* LLM/Large Language Model -> 大语言模型

 \* Zero-shot -> 零样本

 \* Few-shot -> 少样本

 \* AI Agent -> AI 智能体

 \* AGI -> 通用人工智能

 

策略：

分三步进行翻译工作，并打印每步的结果：

1. 根据英文内容直译，保持原有格式，不要遗漏任何信息
2. 根据第一步直译的结果，指出其中存在的具体问题，要准确描述，不宜笼统的表示，也不需要增加原文不存在的内容或格式，包括不仅限于：

 \- 不符合中文表达习惯，明确指出不符合的地方

 \- 语句不通顺，指出位置，不需要给出修改意见，意译时修复

 \- 晦涩难懂，不易理解，可以尝试给出解释

3. 根据第一步直译的结果和第二步指出的问题，重新进行意译，保证内容的原意的基础上，使其更易于理解，更符合中文的表达习惯，同时保持原有的格式不变

 

返回格式如下，"{xxx}"表示占位符：

 

\### 直译

{直译结果}

 

***

 

\### 问题

{直译的具体问题列表}

 

***

 

\### 意译

\```

{意译结果}

\```

 

现在请按照上面的要求从第一行开始翻译以下内容为简体中文：

\```

content

\```

 

**3.** ***\*商业计划制定\****（高级提示策略）

prompt：

\#上下文#

我是卖酒的。我有一个关于客户信息的数据集：[出生年份、婚姻状况、收入、孩子数量、距上次购买的天数、消费金额]。

\#############

\#目标#

我希望你利用这个数据集将我的客户分组，然后告诉我如何针对每个组别开展营销活动。请按以下步骤操作，不要使用代码：

1. 聚类：使用数据集的列对数据集的行进行聚类，使同一聚类中的客户具有相似的列值，而不同聚类中的客户具有明显不同的列值。确保每一行只属于一个聚类。

 

对于找到的每个聚类：

2. CLUSTER_INFORMATION：根据数据集列描述聚类。
3. 聚类名称：解释[CLUSTER_INFORMATION]以获得该聚类客户的简短名称。
4. marketing_ideas：产生向该客户聚类推销我的产品的想法。
5. RATIONALE：解释为什么[MARKETING_IDEAS]与该客户聚类相关且有效。

\#############

\#风格#

商业分析报告

\#############

\#语气#

专业、技术

\#############

\#受众#

我的商业伙伴。让他们相信你的营销策略是经过深思熟虑的，并有充分的数据支持。

\#############

\#回复：markdown报告#

<对于[CLUSTERS]>中的每个聚类

\- 客户聚类：[CLUSTER_NAME］

\- 简介：[CLUSTER_INFORMATION］

\- 营销理念：[RATIONALE］

\- 原因：[RATIONALE]

<附件>

列出属于每个聚类的行号列表，以支持你的分析。使用这些表头：[[CLUSTER_NAME]，行列表]。

\#############

\#开始分析#

如果你能理解，请向我索要我的数据集。

## ***\*4\*******\*.\*******\*2\*******\*知识库问答\****

### **4****.****2****.****1** **QAnything**

#### **4****.****2****.****1****.****1****简介**

QAnything是网易有道推出的本地知识库问答系统，支持任意格式文件或数据库的本地知识库问答系统，可断网一键安装部署使用。置入任何格式（PDF、docx、pptx、jpg、csv、html等）的本地文件，即可获得准确、快速、靠谱的问答体验。

使用界面如下所示：

![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps78.jpg) 

图 27 QAnything使用界面

#### **4****.****2****.****1****.****2****架构介绍**

以下是QAnything的产品架构图：

![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps79.jpg) 

图 28 QAnything架构

在输入阶段，用户可将各种格式的文件上传到后台数据库，文档首先经过编码器编码，以转化为适合模型处理的形式并存入向量数据库。这一步骤是为了确保文档内容能够被后续的生成模型正确理解和利用。

在检索阶段，系统将用户查询（Query）指令编码转化为查询向量，然后在预建向量数据库中对文档进行检索，查找相关的文档和文档片段。QAnything采用了两阶段检索，一阶段嵌入式（embedding）检索，二阶段重排（rerank），embedding和rerank结合的方式解决了知识库数据量大的场景下检索退化的问题。

在生成阶段，LLM根据检索到的文档生成一个回答或者一段文本。

#### **4****.****2****.****1****.****3****优势与特性**

如下图中绿线所示，二阶段rerank重排后能实现准确率稳定增长，即数据越多，效果越好。其使用的检索组件bce-embedding-base_v1和bce-reranker-base_v1的组合有非常强悍的双语和跨语种能力，能消除语义检索里面的中英语言之间的差异，从而实现：

l 强大的双语和跨语种语义表征能力（基于MTEB的语义表征评测指标）。

l 基于LlamaIndex的RAG评测，表现SOTA（基于LlamaIndex的RAG评测指标）。

![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps80.jpg) 

图 29 QAnything二阶段检索的性能优势

### **4****.****2****.****2** **FastGPT**

#### **4****.****2****.****2****.****1****简介**

FastGPT是一个基于LLM大语言模型的知识库问答系统，提供开箱即用的数据处理、模型调用等能力。同时可以通过Flow可视化进行工作流编排，从而实现复杂的问答场景。

![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps81.jpg) 

图 30 FastGPT使用界面

#### **4****.****2****.****2****.****2****架构介绍**

***\*数据存储结构\**** 

在 FastGPT 中，整个知识库由库、集合和数据3部分组成。集合可以简单理解为一个文件。一个库中可以包含多个集合，一个集合中可以包含多组数据。最小的搜索单位是库，也就是说，知识库搜索时，是对整个库进行搜索，而集合仅是为了对数据进行分类管理，与搜索效果无关。

![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps82.jpg) 

图 31 FastGPT数据存储结构

***\*向量存储结构\**** 

FastGPT 采用了PostgresSQL的PG Vector插件作为向量检索器，索引为HNSW。且PostgresSQL仅用于向量检索（该引擎可以替换成其它数据库），MongoDB用于其他数据的存取。

在MongoDB的dataset.datas表中，会存储向量原数据的信息，同时有一个indexes字段，会记录其对应的向量ID，这是一个数组，也就是说，一组向量可以对应多组数据。

在PostgresSQL的表中，设置一个vector字段用于存储向量。在检索时，会先召回向量，再根据向量的ID，去MongoDB中寻找原数据内容，如果对应了同一组原数据，则进行合并，向量得分取最高得分。

![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps83.jpg) 

图 32 FastGPT向量存储结构

#### **4****.****2****.****2****.****3** **优势与特性**

***\*多向量的目的和使用方式\**** 

在一组向量中，内容的长度和语义的丰富度通常是矛盾的，无法兼得。因此，FastGPT 采用了多向量映射的方式，将一组数据映射到多组向量中，从而保障数据的完整性和语义的丰富度。

 

你可以为一组较长的文本，添加多组向量，从而在检索时，只要其中一组向量被检索到，该数据也将被召回。意味着，你可以通过标注数据块的方式，不断提高数据块的精度。

***\*检索方案\**** 

1. 通过问题优化实现指代消除和问题扩展，从而增加连续对话的检索能力以及语义丰富度。
2. 通过Concat query来增加Rerank连续对话的时，排序的准确性。
3. 通过RRF合并方式，综合多个渠道的检索效果。
4. 通过Rerank来二次排序，提高精度。

![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps84.jpg) 

图 33 FastGPT检索方案

***\*搜索模式\**** 

1. 语义检索 

语义检索是通过向量距离，计算用户问题与知识库内容的距离，从而得出“相似度”，当然这并不是语文上的相似度，而是数学上的。

优点是相近语义理解、跨多语言理解（例如输入中文问题匹配英文知识点）、多模态理解（文本，图片，音视频等）

缺点是依赖模型训练效果、精度不稳定、受关键词和句子完整度影响

2. 全文检索 

采用传统的全文检索方式。适合查找关键的主谓语等。

3. 混合检索 

同时使用向量检索和全文检索，并通过RRF公式进行两个搜索结果合并，一般情况下搜索结果会更加丰富准确。

由于混合检索后的查找范围很大，并且无法直接进行相似度过滤，通常需要进行利用重排模型进行一次结果重新排序，并利用重排的得分进行过滤。

4. 结果重排 

利用ReRank模型对搜索结果进行重排，绝大多数情况下，可以有效提高搜索结果的准确率。不过，重排模型与问题的完整度（主谓语齐全）有一些关系，通常会先走问题优化后再进行搜索-重排。重排后可以得到一个0-1的得分，代表着搜索内容与问题的相关度，该分数通常比向量的得分更加精确，可以根据得分进行过滤。

FastGPT 会使用 RRF 对重排结果、向量搜索结果、全文检索结果进行合并，得到最终的搜索结果。

***\*搜索过滤\**** 

1. 引用上限 

每次搜索最多引用n个tokens的内容。

之所以不采用top k，是发现在混合知识库（问答库、文档库）时，不同chunk的长度差距很大，会导致top k的结果不稳定，因此采用了tokens的方式进行引用上限的控制。

2. 最低相关度 

一个0-1的数值，会过滤掉一些低相关度的搜索结果。该值仅在语义检索或使用结果重排时生效。

***\*问题优化\**** 

1. 背景

在 RAG 中，我们需要根据输入的问题去数据库里执行 embedding 搜索，查找相关的内容，从而查找到相似的内容（简称知识库搜索）。

在搜索的过程中，尤其是连续对话的搜索，我们通常会发现后续的问题难以搜索到合适的内容，其中一个原因是知识库搜索只会使用“当前”的问题去执行。看下面的例子：

用户在提问“第二点是什么”的时候，只会去知识库里查找“第二点是什么”，压根查不到内容。实际上需要查询的是“QA结构是什么”。因此我们需要引入一个【问题优化】模块，来对用户当前的问题进行补全，从而使得知识库搜索能够搜索到合适的内容。使用补全后效果如下：

![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps85.jpg) 

图 34 FastGPT问题优化策略

2. 实现方式

在进行数据检索前，会先让模型进行指代消除与问题扩展，一方面可以可以解决指代对象不明确问题，同时可以扩展问题的语义丰富度。你可以通过每次对话后的对话详情，查看补全的结果。

## ***\*4\*******\*.\*******\*3\*******\*大规模集群方案\****

星火大模型推理和训练集群部署支持（13B或65B或175B）预训练、SFT、lora三种训练方式的集群方案；主要是服务器、网络环境、以及大模型服务软件环境等三个方面的集群组合而成；服务器主要是训练的计算节点、推理节点以及存储服务器三种类型的计算服务器；网络环境主要是三种，一种是应用于训练的过程中参数面高速网络环境，另一种是推理涉及业务环境以及管理设备硬件信息网络环境。大模型服务的软件环境主要是推理服务以及分布式训练服务。

集群环境中组网我们分成4种网络环境：包括业务面、存储面、管理面、参数面；4个面的网络。

以下表格是对各个面的用途简述：

| **网络平面** | **用途**                                                     | **端口规划**                                                 |
| ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 参数面网络   | 实现多节点分布式训练时的模型参数同步                         | 需要高带宽的高性能网络，采用双层的Spine-Leaf组网，组成1:1无阻塞网络。由NPU直出的200GE网口接入到Leaf交换机，Spine/Leaf交换机分别以CE16808和CE9860为例。 |
| 存储面网络   | 主要用于访问存储区的高速大带宽互联的存储系统，保证训练时与存储区的高速传输。 | 业务面和存储面使用的是服务器不同的网卡，分别可以是25G/10GE/1GE等，需要具体现场网络情况调整。 |
| 业务面网络   | 主要用于操作系统业务调度与管理，包括推理服务、系统访问、训练服务使用等 |                                                              |
| 管理面网络   | 主要连接服务器BMC网口                                        | 由每个AI计算节点配置1个GE网口接入到GE交换机                  |

 

如图所示整体环境组网图

![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps86.jpg) 

图 35 大模型集群环境组网图



 

# ***\*5训练工具与训练框架\****

## ***\*5\*******\*.1训练工具与流程介绍\****

待训练文档发布后进行内容合并。

## ***\*5.2参数量计算\****

由于当前主流的大模型普遍采用因果解码器架构，因此下面以 LLaMA 模型为范例，深入剖析其参数数量计算方式。

![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps87.jpg) 

图 36 Transformer架构的编码器-解码器结构，选自《Attention Is All You Need》

对于其他模型，其参数量计算算法可参照此方法计算。首先，假设词表大小为**V**，模型包含**L**层解码器，中间状态的维度大小为**H****，**前馈网络层的中间状态维度大小为**H**′。我们主要关注计算以下几个部分的参数量：

l 输入嵌入层. 首先，输入嵌入层（**E** ∈ R **V**x**H****，****E**代表矩阵，R表示实数集，**V**和**H**分别表示维度**V**行**H**列）将词表中的每个单词映射到一个**H**维的向量，因此输入编码层有 **VH**个参数。

l 多头注意力层. 传统的注意力机制部分包含查询（W **Q**∈ R**H**x**H**）、键（W **K**∈ R**H**x**H**）和值（W **V**∈ R**H**x**H**）的线性变换矩阵，每个变换矩阵都包含**H**2个参数，所以这部分需要3 x **H**2。同时还需要一个额外的线性变换来将多头注意力机制的输出拼接后映射成最终输出（W **O**∈ R**H**x**H**）,这又需要**H**2个参数。因此，多头注意力层总共需要4 x **H**2个参数。

l 前馈网络层. LLaMA的前馈网络层由三个线性变换组成，中间有一个非线性激活函数。前两个线性变换（W **U**∈ R**H**x**H**′和W **G**∈ R**H**x**H**′）将输入从**H**维映射到**H**′维，需要 2 × **HH**′个参数；最后一个线性变换（W **D**∈ R**H**′x**H**）将输出从**H**′维映射回**H**维，需要 **HH**′个参数。因此，前馈网络层总共需要 3 × **HH**′ 个参数。

l 归一化层. 每一层解码器还包含两个 RMSNorm 操作，分别用于对多头注意力层和前馈网络层的输入进行归一化处理，共需要 2 × **H**个参数。此外，最后一层的输出也需要进行归一化处理，这又需要额外的**H**个参数。

l 输出层. 最后，LLaMA 的输出层包含一个线性变换（W **L**∈ R**H**x**V**），将解码器的输出映射到词表大小**V**的维度上，使用softmax归一化后预测下一个单词的概率分布。这个线性变换需要 **VH**个参数。

综上所述，累积输入嵌入层、输出层和**L**层解码器每层的多头注意力层、前馈网络层和归一化层，LLaMA 模型的参数量计算公式为：

参数量 = 2**VH**+ **H** + **L** · (4**H**2 + 3**HH**′ + 2**H**). 					(6.1)

以 LLaMA (7B) 为例计算其参数量，给定 **V** = 32000, **L** = 32, **H** = 4096, **H**′ = 11008，

将这些值代入上述公式中：

参数量 = 2 × 32000 × 4096 + 4096 + 32 × (4 × 40962 + 3 × 4096 × 11008 + 2 × 4096)

= 6, 738, 415, 616.													(6.2)

计算得到的参数量与 LLaMA (7B) 模型的实际参数量完全一致。

### **5****.****2.****1****训练运算量估计**

模型训练运算量指的是模型在训练过程中，需要进行的浮点运算次（Floating Point Operations, FLOP）。这里的浮点运算包括浮点数的加减乘除运算，以及浮点数的指数函数，对数函数，三角函数等运算操作。使用 Transformer 架构进行训练的运算量主要集中在多头注意力计算和线性变换计算。相比之下，归一化、输出映射和旋转位置编码计算所需的运算量较少，而输入编码层则无需计算，因此后续的分析中省略了这些部分。在分析多头注意力和线性变换的运算量时，我们进一步设定以下参数：模型总参数量为 **P**，批处理大小为 **B**，输入序列长度为 **T**，因此训练词元总数为 **C** = **BT**；多头注意力机制包含**N**个头，每个头的维度为**D**，因此和中间状态维度**H**满足关系**H****=****ND**。其它定义与参数量计算一节 6.2保持一致。

l 多头注意力. 首先分析多头注意力机制一次计算的运算量。对于批次化的数据，计算得到相应的查询、键和值张量，**Q****,****K****,****V** ∈ R**B**x**T**x**H** ，考虑到需要进行多头计算，这些张量需要进行拆分和转置，得到**Q**′，**K**′，**V**′ ∈ R**B**x**N**x**T**x**D**。在注意力计算中**Q**′**K**′T的矩阵乘法需要2**BT**2**ND**次浮点运算；接着，进行标准化操作（![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps88.jpg)放缩）需要**BT**2**N**次浮点运算，softmax 操作需要进行指数、加和、归一化操作，总计3**BT**2**N**次浮点运算；最后结果与**V**′进行矩阵乘法，再需要2**BT**2**ND**次浮点运算。因此，一次多头注意力计算总的浮点运算量为4**BT**2**ND****+**4**BT**2**N****。**考虑到后向传播的运算量大致为前向传播的两倍，整个模型中多头注意力运算量可表达为：

参数量 =12·(**BT**2**ND** + **BT**2**N****)**·**L** **=** 12**CTL**·(**H** + **N****)**				(6.3)

l 线性变换. 接下来考察线性变换的训练运算量，其包括注意力中的四个映射、前馈网络层的变换以及输出层映射。以前馈网络层中的上映射操作**X**′**W****U**为例，中间状态**X**′∈ R**B**x**T**x**H**，上映射矩阵**W****U**∈ R**H**x**H**′，因此其前向传播需要2**BTHH**′次浮点运算，反向传播则需要4**BTHH**′次浮点运算，总计需要6**CHH**′次浮点运算，其中**C**=**BT**为训练的词元总数。可以看到，线性变换的运算量与矩阵参数量相关，因此Transformer中所有线性变换部分的运算量公式可表达为：

运算量 = 6**C**·线性变换的参数量					(6.4)

若训练过程中采用了激活重计算技术，反向传播时需要额外进行一次前向传播，	则总运算量将变为：

运算量 = 8**C**·线性变换的参数量					(6.5)

最后，通过对比公式6.2、6.3和6.4，可以发现多头注意力的运算量约为线性变换运算量的![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps89.jpg)，考虑到大模型训练场景下序列长度T小于等于中间状态维度H，因此多头注意力运算量最多为线性变换运算量的![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps90.jpg)，其影响相对较小。根据公式6.1，线性变换的参数量通常占总参数量的 95% 以上。因此，可以直接用参数量P替换公式 6.4 中的线性变换的参数量。在这种情况下，参数量为 P 的模型在 C 个词元上进行预训练的总体运算量可以按照下式进行估计: 

运算量 ![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps91.jpg) 6CP									 （6.6）

进一步，如果使用了激活重计算技术，则运算总量8**CP****。**

下面以LLaMA (7B) 的训练为例介绍运算总量的计算方法。其参数量**P** ![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps92.jpg) 6.74×109。这里假设训练数据的词元总数均为 **C** = 1×109，不使用激活重计算技术， 那么 LLaMA (7B) 的训练过程中浮点运算总量为 6 × 6.74 × 109 × 109 ![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps93.jpg) 4.04 × 1019。对于BERT Large而言，它的参数量为 330M，因此训练所需要的浮点运算总量为 6×3.36×108 ×109 ![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps94.jpg) 2.02×1018。

### **5****.****2****.2****训练时间估计**

在训练过程中，训练时间的计算涉及多个部分，主要包括浮点数运算、数据读 写以及多进程同步等。其中，浮点数运算的耗时是训练过程中最主要的部分。因 此，可以根据训练运算量的估计公式(公式 6.6)以及 GPU 的浮点运算能力来大 致估算训练时间。具体的估计公式如下:

训练时间 = ![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps95.jpg)

在这个公式中，GPU 每秒浮点运算数通常是 GPU 理论浮点运算能力的 30% 到 70%，而这一比例通常会受到多种实际因素的影响。以 LLaMA (65B) 的预训练为例，其参数量 **P** = 6.5 × 1010，词元数 **C** = 1.4 × 1012，由于采用了激活重计算技术，其运算量大致为 8**CP** = 7.28 × 1023。它在预训练过程中使用了 2048 张 A100 GPU， 而每张A100 GPU每秒最多能进行 3.12 × 1014 次 BF16 浮点数运算。我们假设在训练过程中，每张 GPU 能达到每秒 2 × 1014 次 BF16 浮点数运算的实际性能。根据上述公式，可以计算出 LLaMA (65B) 使用 2,048 张 A100 GPU 在 1.4T 个词元上 的训练时间大致为 1.78 × 106 秒，即大约为 20.6 天。这个估算结果与论文中公布 的 21 天基本一致。



 

# ***\*6\*******\*大模型发展现状与未来展望\****

## ***\*6\*******\*.\*******\*1\*******\*国内报告分析\****

本小节内容引用了***\*人民网财经研究院\****、***\*至顶科技\****联合发布《开启智能新时代：2024年中国AI大模型产业发展报告》[7]。报告对于AI大模型产业发展背景、产业发展现状、典型案例、挑战及未来趋势等方面进行了系统全面的梳理，为政府部门、行业从业者以及社会公众更好地了解AI大模型产业提供参考。

以下是报告中提取的重点内容：

**1.** ***\*政策、技术、市场驱动\*******\*AI\*******\*大模型产业发展\****

近年来，我国始终高度重视人工智能发展机遇和顶层设计，发布多项人工智能支持政策。国务院于2017年发布《新一代人工智能发展规划》，科技部等六部门于2022年印发《关于加快场景创新 以人工智能高水平应用促进经济高质量发展的指导意见》对规划进行落实。2024年《政府工作报告》中提出开展***\*“人工智能+”\****行动。

伴随人工智能领域大模型技术的快速发展，我国不少地方政府出台相关支持政策，加快大模型产业的持续发展。当前，北京、上海、广东、安徽、福建和深圳、杭州、成都等地均发布了AI大模型的相关产业政策。

《报告》认为，中国AI大模型产业发展源于多领域的广泛需求，例如来自办公、制造、金融、医疗、政务等场景中降本增效、生产自动化、降低风险、提高诊断准确率、提高政务服务效率等诉求。相关领域的创新和发展共同推动着中国AI大模型产业的蓬勃发展，预示着未来更广阔的市场前景。

**2.** ***\*中国\*******\*AI\*******\*大模型产业呈现蓬勃发展的态势\****

《报告》对目前的AI大模型按照部署方式进行了划分，主要分为***\*云侧大模型\****和***\*端侧大模型\****两类。具体而言，云侧大模型分为通用大模型和行业大模型；端侧大模型主要有手机大模型、PC大模型。

伴随多家科技厂商推出的AI大模型落地商用，各类通用、行业以及端侧大模型已在多个领域取得了显著的成果，如在金融、医疗、政务等领域，AI大模型已成为***\*提升服务质量和效率\****的重要手段。

我国具有代表性的通用AI大模型主要包含科大讯飞的***\*讯飞星火认知大模型\****、百度公司的***\*文心一言大模型\****、阿里巴巴的***\*通义千问大模型\****等[7]；行业AI大模型主要涵盖蜜度的文修大模型、容联云的赤兔大模型、用友的YonGPT大模型；同时具有云侧和端侧大模型的端云结合AI大模型主要有vivo的蓝心大模型；端侧AI大模型主要以蔚来的NOMI GPT大模型为代表。

《报告》将vivo蓝心大模型作为中国AI端云结合大模型典型案例，介绍了其端侧化、矩阵化优势。vivo副总裁、vivo AI全球研究院院长周围表示，vivo结合自研大模型端侧化、矩阵化的技术优势并且会聚焦手机行业的应用经验，利用大模型重构手机各类功能，找到落地场景，普惠更多用户。

**3.** ***\*中国\*******\*AI\*******\*大模型产业发展仍存在多方面挑战\****

大模型产业遭遇***\*算力瓶颈\****。随着AI大模型规模呈现指数级增长，训练大模型越发依赖高性能AI芯片。国内AI高性能芯片市场受进口限制和国内技术瓶颈的双重影响，大模型产业发展受到算力层面的一些制约。

主流大模型架构仍存在诸多局限。首先，***\*Transformer\****架构消耗的算力资源普遍较大；其次，基于***\*Transformer\****架构的大模型对存储设备的要求也更高。

高质量的训练数据集仍需扩展。国内的AI大模型数据主要来自***\*互联网、电商、社交、搜索\****等渠道，存在数据类型不全面，信息可信度不高等问题。整体来看，我国可用于大模型训练的中文数据库体量严重不足。

***\*大模型爆款应用尚未出现\****。国内的AI大模型产业至今没有出现爆款级应用，原因在于尚未找到商业化思路，缺乏满足客户需求的个性化应用。我国大模型产业要推出爆款级应用，势必要在应用领域做深做细，让每一个用户都可以充分享受到大模型所带来的真正便利。

**4.** ***\*展望中国\*******\*AI\*******\*大模型四大产业趋势\****

中国信息通信研究院人工智能研究中心副总工程师王蕴韬表示，AI大模型的出现，使得利用人工智能技术来生成内容，从***\*“可用”\****跨越到***\*“好用”\****。未来，人工智能生成内容从***\*“好用”\****到***\*“高效”\****，也许会再经历一次或多次技术范式的颠覆。

同时，《报告》提出了中国AI大模型四大大产业趋势展望：一是***\*AI\*******\*云侧与端侧大模型满足不同需求\****，C端用户将成为端侧的主要客群；二是AI大模型趋于通用化与专用化，***\*垂直行业将是大模型的主战场\****；三是AI大模型将***\*广泛开源\****，小型开发者可调用大模型能力***\*提升开发效率\****；四是AI高性能芯片不断升级，AI大模型***\*产业生态体系将不断完善\****。

《报告》认为，AI大模型可以创造新价值、适应新产业、重塑新动能，是加快发展新质生产力的关键要素。面对未来，我国需进一步加强资源与研发力量的统筹，强化大模型在发展中的场景牵引作用，促进经济社会的高质量发展，以实现大模型技术的高质量应用突破，驱动实体经济的蝶变和产业变革。

## ***\*6\*******\*.\*******\*2\*******\*国际报告分析\****

本小节内容引用了斯坦福大学教授李飞飞领导的团队HAI的最新研究报告《Artificial Intelligence Index Report 2024》[5]。报告指出，人工智能的发展正以惊人的速度向前推进，开发人员每月都在制造出越来越强大、越来越复杂的模型。然而，尽管发展速度加快，***\*人工智能行业在解决人们对人工智能可解释性的担忧以及对其对人们生活影响的日益紧张方面却进展甚微。\****

以下是报告中提取的重点内容：

**1.** ***\*大模型发展速度持续提升\****

2023年的进展速度比以往任何一年都要快得多，GPT-4、Gemini 和 Claude 3 等最先进的系统显示出令人印象深刻的多模态功能，能够生成流畅的数据多种语言的文本、处理音频和图像以及解释网络梗图。

**2.** ***\*大模型发布数量实现翻倍\****

2023年新发布的支持生成式AI的大型语言模型数量比前一年翻了一番，其中***\*三分之二是开源模型\****，例如 Meta的Llama 2，但***\*性能最佳的是闭源模型\****，例如 Google的Gemini Ultra。

**3.** ***\*前沿研究以工业界为主导\****

2023 年，工业界继续主导人工智能前沿研究。工业界产生了 51 个值得关注的机器学习模型，而学术界仅贡献了 15 个。2023 年，产学界合作产生了 21 个值得关注的模型，再创新高。

**4.** ***\*顶级\*******\*AI\*******\*模型主要来源地为美国\****

美国领先中国、欧盟和英国，成为顶级人工智能模型的主要来源地。2023 年，61 个著名的人工智能模型源自美国机构，远远超过欧盟的 21 个和中国的 15 个。

**5.** ***\*大模型能力首次超越人类\****

Gemini Ultra是第一个在大规模多任务语言理解关键基准测试中达到人类水平表现的 LLM。OpenAI的GPT-4 也不甘示弱，在 Holistic Evaluation of Language Models 基准上取得了 0.96 的平均胜率得分，该基准将 MMLU 与其他评估结合起来。

**6.** ***\*人工智能模型研发成本不断上升\****

不过，人工智能性能的提高是有代价的，报告发现，前沿人工智能模型的开发成本正变得越来越高。据说 Gemini Ultra 消耗了价值 1.91 亿美元的计算资源，而 GPT-4 的开发成本估计为 7800 万美元。

## ***\*6\*******\*.\*******\*3\*******\*人物观点\****

### **6****.****3****.****1Sam** **Altman**

2024年4月25日，OpenAI CEO奥特曼（Sam Altman）在斯坦福大学的英伟达礼堂进行了一场公开演讲，向在场的1000多名与会者分享了他对人工智能未来的洞见。演讲中提到的观点如下：

**1.** ***\*GPT\*******\*-\*******\*5\*******\*，将会比现在的大模型更智能\****

l Sam Altman 坚定地认为，根据科学预测，GPT-5 将比 GPT-4 智能得多，而 GPT-6 的智能又会远超GPT-5。目前OpenAI还没有达到这个智能发展曲线的顶点。

l “抄袭”很容易，在GPT-4 诞生的基础上，Google 很容易“复制”出 Gemini，但真正的创新在于界定 AI 能力的下一个范式转变。

l Sam Altman 将AI的潜力与 iPhone 对移动计算的变革性影响相提并论。

l OpenAI 的使命是实现AGI，开源可能不是实现这一目标的最好途径，OpenAI 希望通过向公众提供免费无广告的 ChatGPT 来实现社会影响力。

**2.** ***\*追求财务收益，也严肃对待使命\****

l 无论 OpenAI 资金投入多少，关键是能否持续地为社会创造超出这些投资的价值，并找到支付这些费用的途径。

l 赚钱和资本主义是正面的，但合作伙伴在追求财务利益的同时，也严肃对待使命。为了确保这一点。OpenAI 建立了确保激励措施一致性的机制。

**3.** ***\*不用担心\*******\*AI\*******\*太强，人类更喜欢人类\****

l Sora 的训练数据用了多少 YouTube 的视频？Sam Altman：让我们跳过这个问题。

l 不担心 AI 太强，GPT-4 一出来，人们觉得变天了，但现在人们的态度变成了“GPT-4 真糟糕，GPT-5 在哪里呢？”

l 人类更喜欢人类，即便现在 AI 下棋吊打人类，人类还是只喜欢看人类下棋，不过 Sam Altman 也指出一些反例，比如青少年更喜欢跟 AI 聊而不是跟心理医生聊。

l 意识到 Scaling Law 很重要之后，OpenAI 把所有组的计算资源都聚集起来做一件事。

l 不需要全新的数据也能让模型推理能力不断提升。

**4.** ***\*不是新的生物，而是辅助人类的工具\****

l AI 的普及和低价有望消除不公，为全球带来益处。

l 现在做 AI 初创是一个很好的时机，但不会因为用上了 AI 就超过目前已有的产品，AI 并不会违背商业的定律。

l 很多人的创业/研究方向是在补全现在 AI 的缺陷，本质是在赌 AI 不会变得更好，但未来的 GPT-5、6 会让这些努力变得没意义。

l Sora 会实现全新的娱乐方式，每次会根据你的喜好、互动实时生成不同的东西，会从电影和游戏之间诞生另一个生态。

l OpenAI 在做的是辅助人类的工具，而不是新的生物，所以不觉得 ChatGPT 需要有情感。

l 未来将会诞生一个高薪工作，专门负责给模型生成的结果提供专业反馈。

**5.** ***\*AI\**** ***\*抑制创新？人类会创造更多惊喜\****

l 秘密开发通用人工智能（AGI）不是“好邻居”的行为，而应该让社会与科技同步发展，让社会参与决定他们希望从技术中得到什么。

l 不用担心 AI 会抑制人类的创新能力，反而相信人们会利用更先进的工具创造出更多惊喜，历史已经证明，当人们得到更多支持时，他们会创造出更多令人惊叹的成就。

l OpenAI有责任为现在和未来的人类创造一个更好的世界。AGI 有潜力替代某些专业职业如律师和医生，使得法律和医疗服务更加普及，特别对世界较贫穷的一半人口的帮助将大于对较富裕人口的帮助。

### **6****.****3****.****2****吴恩达**

2024年3月22日，斯坦福大学教授Andrew Ng（吴恩达）在由红衫资本举办的人工智能峰会（AI Ascent）上的演讲报告中提到，“我认为AI智能体工作流程将在今年推动AI的大规模进步，甚至可能比下一代基础模型还要多。这是一个重要的趋势，我敦促所有从事人工智能工作的人都关注它。”

吴恩达在此次演讲中，主要介绍了AI Agent如何使用代理工作流来显著提高AI的回复质量。并表示好的Agent工作流可以让3.5模型效果高于4.0的回复。

以下是吴恩达提出的四种AI Agent设计模式，包括：

**1.** ***\*检查（\*******\*Reflection\*******\*）\****：LLM自我检查以提出改进方法。

**2.** ***\*工具使用（\*******\*Tool\**** ***\*use\*******\*）\****：LLM拥有网络搜索、代码执行或任何其他功能来帮助其收集信息、采取行动或处理数据。

**3.** ***\*规划（\*******\*Planning\*******\*）\****：LLM提出并执行一个多步骤计划来实现目标，或过程中执行复杂的规划算法，如失败规避等。

**4.** ***\*多智能体协作（\*******\*Multiagent\**** ***\*collaboration\*******\*）\****：多个不同的Agent协作，分配任务并讨论、辩论，以提出比单个Agent更好的解决方案。

***\*1\*******\*. 检查（\*******\*Reflection\*******\*）\****

即让AI来检查AI的输出，举例：

Step 1：你是一名专业的Python研发人员，你现在正在写一个脚本，该脚本可以自动识别world文件、pdf文件里的第一行文本，并把该文本用作文件的文件名。

Step 2: 你把写好的脚本给到了你的上司，一位资深的Python研发专家。他审查了你的代码，对性能、安全性和结构的全面评估，给出了修改建议。

Step 3: 你根据上司的建议，修改了代码并输出。这种输出代码的质量，比你定义一个角色区输出效果要好很多。而且它还能规避很多你意想不到的问题。

***\*2\*******\*. 工具使用（\*******\*Tool\**** ***\*use\*******\*）\****

要善于使用各种生产力工具：

比如，编码可以使用copilot。在GPT plus里就是各种插件。比如做数据分析的插件，做网络搜索的插件等。或者是说可以让AI运用已经很成熟的一些理论公式之类。这样输出效果也会很好。还比如让AI运用SWOT分析法分析某个行业。

***\*3\*******\*. 规划（\*******\*Planning\*******\*）\****

规划就是把一个复杂的事情分拆成多个步骤去执行。

Request:Please generate an image where a girl is reading a book,and her pose is the same as the boy in the image example.jpg,then please describe the new image with your voice.

大致意思为识别图片中男孩的姿势，然后生成一张女孩在读书的图。女孩的姿势和男孩一样。最后用语音描述这幅新生成的图片。这个在一个AI工具里是做不到的。但是在comfyui这类集成工具里是可以做到的。

***\*4\*******\*. 多智能体协作（\*******\*Multiagent\**** ***\*collaboration\*******\*）\****

举个例子：请你扮演一个电商公司的2个不同角色，一个名字叫张三是运营总监，一个名字叫李四是产品总监。

step 1：张三先提出一个创意：搞一个拉新比赛，奖金1万元，给拉新人数最多得5000，2-3名平分3000，4-10名平分2000。

step 2 : 李四拿到方案后，给出反馈意见，并且给出优化后的作品。

不断重复这个过程，至少双方有5次相互反馈和优化的过程。最终输出一个完整的，在预算范围内，能最大程度获得最多注册量的营销方案。你会发现，经过多轮来回沟通完善后，这个输出的方案会比你最开始的方案要好很多。而且，我们还可以定义更多的角色参与这个过程。

## ***\*6\*******\*.\*******\*4\*******\*风险与伦理\****

在大模型技术发展过程中，可能遇到的风险和伦理问题包括（由星火生成）：

l 隐私与数据安全：随着大模型对大量数据的依赖性增加，如何确保用户数据的隐私和安全性成为了一个重大挑战。这涉及到如何处理和存储敏感信息，以及如何防止数据泄露或滥用。

l 偏见与公平性：大模型可能会无意中从训练数据中继承并放大现有的社会偏见。因此，开发无偏见的算法和确保模型的公平性是重要的伦理考量。

l 自动化决策的道德责任：当AI系统（特别是大模型智能体）被用于关键决策时，确定在出现问题时应由谁承担责任成为复杂问题。这涉及到AI系统的透明度、可解释性和责任归属问题。

l 人工智能取代人类工作：随着AI技术的发展，尤其是在自动化和智能化领域，可能会导致某些职业的消失，引发就业市场的变化和社会结构的调整。

l 武器化风险：大模型技术在军事和安全领域的应用可能带来新的风险，例如自主武器系统的发展可能导致战争方式的改变，增加冲突的可能性。

l 人机关系：随着大模型技术的普及，人与机器之间的互动变得更加频繁和深入，如何处理这种关系，以及如何在人机交互中保持人类的主导地位，也是一个需要深思的问题。

l 知识产权和版权：大模型在创作内容（如音乐、文本、艺术作品等）时可能会产生新的知识产权问题，特别是在AI生成内容的版权归属上存在争议。

l 法律监管滞后：由于AI技术的快速发展，现有的法律法规可能无法充分预见并覆盖所有潜在的风险和伦理问题，导致法律监管在某些情况下跟不上技术发展的步伐。

这些风险和伦理问题需要在大模型的开发和应用过程中予以重视，并通过政策制定、技术标准设定以及公众教育等方式加以解决和缓解。





# ***\*7\*******\*团队学习与发展\****

## ***\*7\*******\*.\*******\*1\*******\*学习资源\****

### **7****.****1****.****1****热门论文/文章精选**

LLM领域相关论文可以帮助更能快地了解最新研究进展，掌握最新的技术和理论。本次报告共收录了arxiv.org上大模型Top热门论文70篇。

论文PDF资源同步更新到了可交付资产中心，可访问进行资源下载：http://wiki.kxdigit.com/ （需申请办公网VPN）。

![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps96.jpg) 

图 37 LLM精选论文收录页面

### **7****.****1****.****2****课程精选**

课程精选是一系列精心挑选的LLM相关课程，旨在帮助团队成员理解和掌握大模型的关键概念、技术和应用，包括模型训练、微调、优化和部署等方面。

相关视频课程资源同步更新到了可交付资产中心，可访问进行资源下载：http://wiki.kxdigit.com/ （需申请办公网VPN）。

 

![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps97.jpg) 

图 38 LLM视频课程收录页面

### **7****.****1****.****3****大模型相关资源下载**

LLM相关资源主要包括目前热门的预训练模型、训练数据、模型代码、相关工具等。以下是资源收录时常见的下载来源：

l 预训练模型：许多研究团队和组织会公开分享他们的预训练模型。例如，Hugging Face的模型库包含了大量的预训练模型，如Llama 3、BERT等。

l 训练数据：公开的数据集是训练和评估模型的重要资源。例如，GLUE和SuperGLUE是一组用于自然语言理解任务的基准数据集。

l 模型代码：公共代码仓库如GitHub、Gitee。

l 相关工具：训练和使用大模型需要一些特定的工具，如TensorFlow、PyTorch等深度学习框架，以及Hugging Face的Transformers库，可部署和训练工具等。

以上资源同步更新到了可交付资产中心，可访问进行资源下载：http://wiki.kxdigit.com/ （需申请办公网VPN）。

 

![img](file:///C:\Users\zjma6\AppData\Local\Temp\ksohtml46276\wps98.jpg) 

图 39 LLM相关资源收录页面

## ***\*7\*******\*.\*******\*2\*******\*培训计划\****

2024年第二期训练营中安排一场大模型交付相关课程。





# ***\*8\*******\*总结\****

本次季度报告全面总结了LLM大模型技术的关键趋势和发展，强调了整理知识的重要性，旨在为团队成员提供深入理解当前及未来人工智能领域所需的全面技术视角。通过系统地分析和总结大模型技术，我们能够加强内部技术共识、促进知识共享，并显著提升团队在面对复杂问题时的解决方案设计能力、创新能力和项目执行效率，从而在激烈的市场竞争中保持领先地位。

具体而言，报告聚焦于权威的大模型理论背景知识、最新的大模型前沿技术、应用案例。涵盖了包括从卷积神经网络到LLM Agent智能体等在内的关键技术范畴，此外还列举了大模型热门的应用案例，探讨了大模型当前和未来的技术动态和技术创新，实际指导了大模型训练中的框架工具运用全流程，以及相关优化策略等，全方位地对大模型进行了讲解，以实现从“认识大模型”到“使用大模型”的这一能力提升目标。在时间周期上，报告回顾了过去一个季度的技术发展，并展望了一至两年内的技术趋势与潜在挑战，确保团队能够及时更新技术视角并做出战略性的长远规划。

针对下一季度的学习和发展，建议团队应持续关注AI大模型技术的动态变化，特别是预训练语言模型的最新进展。考虑到计算资源的消耗和技术实施的挑战，如多智能体交互机制的效率问题，团队需要探索如何优化现有模型以提高资源效率。同时，鼓励团队成员积极参与开源项目和社区讨论，以促进知识共享和技术创新。通过这些行动，我们团队将更好地准备迎接AI大模型技术的未来挑战。

 

 

 

 

 

 

 

 

 

 

# ***\*参考文献\****

[1] Wayne Xin Zhao et al. A Survey of Large Language Models. In: arXiv preprint arXiv: 2303.18223 (2023).

[2] Frederick Jelinek. Statistical Methods for Speech Recognition. MIT Press, 1998.

[3] 赵鑫、李军毅、周昆、唐天一、文继荣. 大语言模型. RUC AI BOX Press,2024.

[4] 宗成庆. 统计自然语言处理. 清华大学出版社, 2013.

[5] Yoshua Bengio et al. A Neural Probabilistic Language Model. In: JMLR (2003). 

[6] 邱锡鹏. 神经网络与深度学习. 机械工业出版社, 2020

[7] Ashish Vaswani et al. Attention is All you Need. In: NIPS. 2017.

[8] Kim Martinneau. What is retrieval-augmented generation[W]. Retrieved from IBM Press. https://research.ibm.com/blog/retrieval-augmented-generation-RAG, 2023.

[9] AWS Team. 中小企业如何借助 RAG 技术无代码快速搭建私域知识库应用[W]. 亚马逊AWS官方博客，2023. 

[10] 张奇、桂韬、郑锐、黄萱菁. 大规模语言模型：从理论到实践. 中国工信出版集团， 电子工业出版社, 2023.

[11] Aidan Wilson. A Brief Introduction to Unsupervised Learning[W]. Retrieved from https://towardsdatascience.com, 2020.

[12] Dmytro Nikolaiev. Parameters in Transformer models, An inside look at the Transformer Encoder/Decoder building blocks[W]. Retrieved from https://towardsdatascience.com, 2023.

[13] OpenAI. GPT-4 Technical Report[R]. OpenAI. Retrieved from https://cdn.openai.com/papers/gpt-4.pdf , 2023.

[14] HAI. Artificial Intelligence Index Report 2024[R]. HAI. Retrieved from https://hai.stanford.edu/research/ai-index-report, 2024.

[15] 维基百科. wikipedia[w]. Retrieved from https://en.wikipedia.org.

[16] 人民网财经研究院.开启智能新时代：2024年中国AI大模型产业发展报告[R]. people.cn. Retrieved from http://lib.ia.ac.cn/news/newsdetail/68799, 2024.

[17] 科大讯飞公众号. 讯飞星火大模型V3.5春季上新[W].科大讯飞.2024. 

[18] Reiichiro Nakano et al. WebGPT: Browser-assisted question-answering with human feedback. In: arXiv preprint arXiv:2112.09332 (2021).

[19] Sirui Hong et al. MetaGPT: Meta Programming for Multi-Agent Collaborative Framework. In: arXiv preprint arXiv:2308.00352 (2023).

[20] Joon Sung Park et al. Generative Agents: Interactive Simulacra of Human Behavior. In: arXiv preprint arXiv:2304.03442 (2023).

[21] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. In: arXiv preprint arXiv:2303.11366 (2023).


